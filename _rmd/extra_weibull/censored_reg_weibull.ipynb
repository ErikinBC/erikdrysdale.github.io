{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitted regularized censored regression models for Weibull or Exponential distributions\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\bZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\bz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\bt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\biota}{\\boldsymbol{\\iota}}\n",
    "\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}\n",
    "$$\n",
    "\n",
    "There are many situations when the observed data is censored, meaning that its value is only *partially* known. For example, income data which is published by national statistics agencies will truncate the ganularity of the data to some upper bound (greater than \\$500K dollars for example in order to preserve the privacy of high-income households). In the biomedical domain in which I work in, certain biological measurements are not measureable if there are outside some detectable range. In both cases, the data we record is partially known because we know the measurement was *at least* this large, also known as right-censoring. The canonical case of right-censoring is survival times in which a patient has lived for *at least* this long. When a measurment is *at most* some value, this is known as left censoring.\n",
    "\n",
    "In this post I will show how to build a regularized linear model from scratch in `python` to be able to fit right-censored data with an explicit likelihood model. The data challenge I have in mind is the one of missing value imputation for censored data; namely we want to predict the expected value of a censored measurement observation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution\n",
    "\n",
    "Recall that the likelihood of a sample of $n$ i.i.d. exponential distributions can be written: $f(t_i;\\lambda)=\\lambda \\exp(-\\lambda t_i)$, $S(t_i;\\lambda)=\\exp(-\\lambda t_i)$, and $h(t_i; \\lambda) = \\lambda)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(\\bbeta) &=  \\prod_{i=1}^n \\hspace{1mm} f(\\bbeta;t_i)^{\\delta_i} S(\\bbeta;t_i)^{1-\\delta_i}  \\\\\n",
    "&= \\prod_{i=1}^n \\hspace{1mm} h(\\bbeta; t_i)^{\\delta_i} S(\\bbeta;t_i) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence the log-likelihood is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell(\\bbeta) &= \\sum_{i=1}^n \\big[ \\delta_i \\log h(\\bbeta; t_i) + \\log S(\\bbeta;t_i) \\big]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "As we have parameterized $h(\\bbeta; t_i) = \\lambda_i = \\exp(\\bx^T \\bbeta)$, where $\\bx^T \\bbeta = \\sum_{j=0}^p \\beta_jx_j$, and $S(\\bbeta; t_i) = \\exp(- \\lambda_i t_i)= \\exp(-\\exp(\\bx^T \\bbeta) t_i)$, we can defined the log-likelihood as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell(\\bbeta) &= \\sum_{i=1}^n \\big[ \\delta_i (\\bx_i^T \\bbeta ) - \\exp(\\bx_i^T \\bbeta ) t_i \\big] \\\\\n",
    "\\frac{\\partial \\ell(\\bbeta)}{\\partial \\beta_j} &= \\sum_{i=1}^n \\big[ \\delta_i x_{ij} - z_i x_{ij} \\big] = \\bX_j^T(\\bdelta - \\bz) , \\hspace{3mm} z_i = \\exp(\\bx_i^T \\bbeta) t_i \\\\\n",
    "\\frac{\\partial ^2 \\ell(\\bbeta)}{\\partial \\beta_j \\partial \\beta_k} &= - \\sum_{i=1}^n x_{ij}x_{ik} z_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "While the partial derivative were derived for the likelihood, as we will use function minimizers, the vectorized gradient adn Hessian below are derived for the negative log-likelihood (simply a sign change):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_\\bbeta &= - \\bX^T (\\bdelta - \\bz) \\\\\n",
    "H_\\bbeta &=  \\bX^T \\bZ \\bX, \\hspace{2mm} \\text{diag}(\\bZ)_i = [\\exp(\\bx_i^T \\bbeta) t_i ]\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively re-weighted least squares\n",
    "\n",
    "The Newton-step update at the $t+1$ iteration can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bbeta^{(t+1)} &\\gets \\bbeta^{(t)} - (H_{\\bbeta^{(t)}})^{-1} \\nabla_{\\bbeta^{(t)}} \\\\\n",
    "&\\gets \\bbeta^{(t)} + (\\bX^T \\bZ^{(t)} \\bX )^{-1} \\bX^T (\\bdelta - \\bz^{(t)}) \\\\\n",
    "&\\gets (\\bX^T \\bZ^{(t)} \\bX )^{-1} \\bX^T \\bZ^{(t)} \\by^{(t)}, \\hspace{2mm} \\by^{(t)} = \\bX \\bbeta^{(t)} + (\\bZ^{(t)})^{-1} (\\bdelta - \\bz^{(k)})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The last line reveals that the Newton update for the Exponential likelihood model is equivalent to a weighted least-squares problem.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bbeta^{(t+1)} &= \\arg\\min_{\\bbeta} \\hspace{2mm} (\\by^{(t)} - \\bX \\bbeta)^T  \\bZ^{(t)} (\\by^{(t)} - \\bX \\bbeta)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Hence if our original likelihood model has some elastic-net regularization of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell_{\\gamma}(\\bbeta) &= \\ell(\\bbeta) - \\gamma \\|\\bbeta_{-0}\\|_1 ,\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where the L1-norm penalty is put on the non-intercept coefficient, then the Newton update will be equavilent to solving a least-squares Lasso problem at each iteration.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bbeta^{(t+1)} &= \\arg\\min_{\\bbeta} \\hspace{2mm} (\\by^{(t)} - \\bX \\bbeta)^T  \\bZ^{(t)} (\\by^{(t)} - \\bX \\bbeta) + \\gamma \\|\\bbeta_{0}\\|_1  \\label{eq:irls}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The `sklearn` package has very fast solvers for the type of optimization problem seen in $\\eqref{eq:irls}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weibull model\n",
    "\n",
    "In some cases the exponential distribution not be flexible enough to handle different types of censoring processes. For example the exponential distribution has a hazard rate which is constant across time (or whatever unit $t_i$ stands for). The Weibull distributions allows for a measurement-level dependent hazard with an additional shape parameter ($\\alpha$) in addition to the rate parameter ($\\lambda$). The case a simple rate/shape Weibull distribution has the following density, inverse CDF, and hazard function: $f(t_i;\\lambda,\\alpha) = \\alpha\\lambda t_i^{\\alpha-1}\\exp(-\\lambda t_i^\\alpha)$, $S(t_i;\\lambda,\\alpha)=\\exp(-\\lambda t_i^\\alpha)$, and $h(t_i;\\lambda, \\alpha) = \\alpha \\lambda t_i^{\\alpha-1}$. When $\\alpha=1$, the Weibull distribution reduces to the Exponential making the latter a nested distribution. \n",
    "\n",
    "In the case of $n$ data points with a $p$-dimensional covariate vector we will again parameterize the scale parameter as $\\lambda_i = \\exp(\\bx_i^T \\bbeta)$, and leave the shape parameter as a global term. Because $\\alpha \\in (0,\\infty)$, it will be easier to optimize over $\\alpha(\\phi)=\\exp(\\phi)$ so that $\\phi \\in (-\\infty,\\infty)$. The log-likelihood funtion then becomes:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\ell(\\bbeta, \\phi) &= \\sum_{i=1}^n \\big\\{ \\delta_i\\big(\\phi + (\\bx_i^T\\bbeta)+[\\exp(\\phi)-1]\\cdot \\log t_i \\big) - \\exp(\\bx_i^T\\bbeta)t_i^{\\exp(\\phi)} \\big\\} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The partial derivatives for both components can be calculated.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ell(\\phi;\\bbeta)}{\\partial \\phi} &= \\sum_{i=1}^n \\delta_i + e^\\phi \\sum_{i=1}^n \\log t_i \\Big[1 - \\exp(\\bx_i^T \\bbeta) t_i^{\\exp(\\phi)} \\Big] \\\\\n",
    "\\frac{\\partial \\ell(\\bbeta; \\phi)}{\\partial \\bbeta} &= \\bX^T( \\bdelta - \\bz_\\phi), \\hspace{2mm} z_{i,\\phi} = \\exp(\\bx_i^T\\bbeta) t_i^{\\exp(\\phi)} \\\\\n",
    "\\frac{\\partial^2 \\ell(\\bbeta; \\phi)}{\\partial \\bbeta \\partial \\bbeta^T } &= - \\bX^T \\bZ_\\phi \\bX\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In order to optimize this likelihood, a block-coordinate descent will prove useful where the parameter $\\phi$ is updated first through a root finding method, and then $\\bbeta$ is solved for with the IRLS approach highlighted above.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi^{(t+1)} &\\gets \\arg \\min \\sum_{i=1}^n \\delta_i + e^\\phi \\sum_{i=1}^n \\log t_i \\Big[1 - \\exp(\\bx_i^T \\bbeta) t_i^{\\exp(\\phi)} \\Big] \\\\\n",
    "\\bbeta^{(t+1)} &\\gets (\\by^{(t)} - \\bX \\bbeta)^T  \\bZ^{(t)}_\\phi (\\by^{(t)} - \\bX \\bbeta) + \\gamma \\|\\bbeta_{-0}\\|_1, \\hspace{2mm} \\by^{(t)} = \\bX \\bbeta^{(t)} + (\\bZ^{(t)}_\\phi)^{-1} (\\bdelta - \\bz^{(k)}_\\phi) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

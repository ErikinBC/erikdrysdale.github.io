---
title: "Notes on Multitask learning"
output: html_document
---

## (1) Stratified L1/L2 multitask learning

Consider the general L2 penalized multitask learning function to be minimized, where there are $D$ different datasets, and $p$ total parameters $\beta \in \mathbb{R}^p$. Each dataset has a loss function $l^{(d)}(\cdot)$, and an associated parameter vector $\beta^{(d)}$, which may be a subset of the parameter space: $\beta_d \subseteq \beta$.

$$
\begin{align*}
l_M(\beta) &= \frac{1}{N} \sum_{d=1}^D \Bigg\{ \pi^{(d)} \cdot l^{(d)}(\beta^{(d)})  \Bigg\} + \frac{1}{2} \lambda \|\Gamma \beta \|_2^2 \\
\pi^{(d)} &= \frac{N^{(d)}}{N}, \hspace{3mm} N = \sum_{i=1}^D N^{(d)}
\end{align*}
$$

The Tikhonov matrix $\Gamma$ is needed in order to ensure that coefficients are not unduly regularized simply because they are only found in a subset of loss functions. Specifically define:

$$
\begin{align*}
\Gamma &= \text{diag}\Bigg(\frac{N_{\beta_1}}{N},\dots,\frac{N_{\beta_p}}{N} \Bigg)^{1/2} \\
\|\Gamma \beta \|_2^2 &= \sum_{j=1}^p \frac{N_{\beta_j}}{N} \beta_j^2 \\
N_{\beta_j} &= \sum_{d \in S_j} N^{(d)}, \hspace{2mm} S_j = \{i: \beta_j \in \beta^{(i)} \}
\end{align*}
$$

For example, suppose there were two datasets $\{1,2\}$ and three parameters $\beta=(\beta_1,\beta_2,\beta_3)'$ of the following form:

$$
\begin{align*}
l_M(\beta) &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot l(\beta^{(d)})  \Bigg\} +  \frac{\lambda}{2} \sum_{j=1}^3 \frac{N_{\beta_j}}{N} \beta_j^2 \\
\beta^{(1)} &= (\beta_1,\beta_2,\beta_3)^T \\
\beta^{(2)} &= (\beta_1,\beta_2)^T \\
N_{\beta_1} &= N, \hspace{2mm} N_{\beta_2} = N, \hspace{2mm} N_{\beta_3} = N^{(1)}
\end{align*}
$$

We can consider the KKT conditions to see why this form of Tikhonov regularization is useful:


$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta_1} &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_1} l(\beta^{(d)})  \Bigg\} + \lambda \beta_1 \\
\frac{\partial l_M(\beta)}{\partial \beta_2} &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_2} l(\beta^{(d)})  \Bigg\} + \lambda \beta_2 \\
\frac{\partial l_M(\beta)}{\partial \beta_3} &= \frac{1}{N} \sum_{d=1}^1 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_3} l(\beta^{(d)})  \Bigg\} + \big(\pi^{(1)}\big) \lambda \beta_3
\end{align*}
$$

### (1.A) Extension to elastic net type penalty

We can easily add on the L1 lasso penalty term:

$$
\begin{align*}
l_M(\beta) &= \frac{1}{N} \sum_{d=1}^D \Bigg\{ \pi^{(d)} \cdot l^{(d)}(\beta^{(d)})  \Bigg\} + \frac{1}{2} (1-\alpha)\lambda \|\Gamma \beta \|_2^2 + \alpha \lambda \|\Gamma^2\beta \|_1
\end{align*}
$$



This helps to ensure that the chosen $\lambda$ does not unnessarily shrink the coefficients simply because they are only found in a subset of the loss functions, helping to get a more accurate global fit. The more general formulation of the KKT condition will be:

$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta_j} &= \frac{1}{N} \sum_{d \in S_j} \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_j} l^{(d)}(\beta^{(d)})  \Bigg\} + \Bigg( \sum_{d \in S_j}  \pi^{(d)}\Bigg) \Bigg[ (1-\alpha) \lambda \beta_j + \alpha \lambda \partial_{\beta_j} |\beta_j |  \Bigg] \\
&\text{Solving this equation w.r.t zero allows us to divide both sides:} \\
\frac{\partial l_M(\beta)}{\partial \beta_j} &= \frac{1}{N} \Bigg( \sum_{d \in S_j}  \pi^{(d)}\Bigg)^{-1} \sum_{d \in S_j} \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_j} l^{(d)}(\beta^{(d)})  \Bigg\} + (1-\alpha) \lambda \beta_j + \alpha \lambda g_j \\
&= \frac{1}{N} J_{\beta,j} + P_{\beta,j} \\
\nabla l_M(\beta) &= \frac{1}{N} J_{\beta} + P_{\beta}
\end{align*}
$$

Where $g_j$ is the subgradient for the absolute value function. To vectorize this, we'll need to introduce some additional notation:

$$
\begin{align*}
&\text{For the likelihood term} \\
\\
l(\beta) &= [l^{(1)}(\beta^{(1)}),\dots,l^{(D)}(\beta^{(D)})] \in \mathbb{R}^{1 \times D} \\
\\
\frac{\partial l(\beta)}{\partial \beta} &= \nabla_{\beta} l(\beta) = \begin{bmatrix} 
\frac{\partial l^{(1)}(\beta^{(1)})}{\partial \beta_1} & \cdots & \frac{\partial l^{(D)}(\beta^{(D)})}{\partial \beta_1} \\
\vdots & & \vdots \\
\frac{\partial l^{(1)}(\beta^{(1)})}{\partial \beta_p} & \cdots & \frac{\partial l^{(D)}(\beta^{(D)})}{\partial \beta_p}
\end{bmatrix} \in \mathbb{R}^{p \times D} \\
\\
[S]_{i,j} &= I(j \in S_i), \hspace{2mm} S \in \{0,1 \}^{p\times D} \\
\\
\Pi &= \iota_p \otimes \pi^T \in [0,1]^{p \times D}, \hspace{2mm} \pi = [\pi^{(1)},\dots,\pi^{(D)}]^T \\
\\
\Omega &= (S \circ \Pi) \iota_D \in [0,1]^{p \times 1} \\
\\
J_\beta &= \Omega^{-1} \circ \big(\nabla_\beta l(\beta) \circ S \circ \Pi \big) \iota_D \\
\\
&\text{For the penalized term} \\
P_\beta &= (1-\alpha)\lambda\beta + \alpha\lambda g
\end{align*}
$$

Using our specific example in the beginning:

$$
\begin{align*}
S_1 &= \{1,2 \}, S_2 = \{1,2\}, S_3 = \{1\} \\
\nabla_\beta l(\beta) &= \begin{pmatrix}
\nabla_1^{(1)} & \nabla_1^{(2)}  \\
\nabla_2^{(1)} & \nabla_2^{(2)} \\
\nabla_3^{(1)} & \nabla_3^{(2)}
\end{pmatrix}, \hspace{3mm} 
S = \begin{pmatrix}
1 & 1  \\
1 & 1 \\
1 & 0
\end{pmatrix}, \hspace{3mm} \Pi = \begin{pmatrix}
\pi^{(1)} & \pi^{(2)} \\
\pi^{(1)} & \pi^{(2)} \\
\pi^{(1)} & \pi^{(2)}
\end{pmatrix}, \hspace{3mm} 
\Omega = \begin{pmatrix}
\pi^{(1)} + \pi^{(2)} \\
\pi^{(1)} + \pi^{(2)} \\
\pi^{(1)}
\end{pmatrix} \\
J_\beta &= \begin{pmatrix}
\big(\pi^{(1)} + \pi^{(2)}\big)^{-1} [\pi^{(1)}\nabla_1^{(1)} + \pi^{(2)}\nabla_1^{(2)}] \\
\big(\pi^{(1)} + \pi^{(2)}\big)^{-1}[\pi^{(1)}\nabla_2^{(1)} + \pi^{(2)}\nabla_2^{(2)}] \\
\big(\pi^{(1)}\big)^{-1}[\pi^{(1)}\nabla_3^{(1)}] 
\end{pmatrix} \\
P_\beta &= \begin{pmatrix}
(1-\alpha)\lambda \beta_1 + \alpha\lambda g_1 \\
(1-\alpha)\lambda \beta_2 + \alpha\lambda g_2 \\
(1-\alpha)\lambda \beta_3 + \alpha\lambda g_3
\end{pmatrix}
\end{align*}
$$

Putting it all together, we can write the (sub) gradient of our multitask loss function in vectorized form:

$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta} &= \frac{1}{N} J_\beta \iota_p + P_\beta \iota_p
\end{align*}
$$

### (2.A) Transfer learning approach #1

Now we'll define $T$ to refer dataset of interest (the target) and $S$ to refer to all other datasets. We can then apply a scaling factor $\tau$ to represent how close we want to be to the multitask situation where $\tau=0$ is pure multitask and $\tau=1$ is only estimating our target dataset: $\pi^{(S)}(1-\tau) + \pi^{(T)} = 1$. All that changes is the $\pi$ vector.

$$
\begin{align*}
\pi &= \begin{bmatrix} (1-\tau)\pi^{(1)} & \cdots  & (1-\tau)\pi^{(D_1)} & 1-(1-\tau)\pi^{(T)} \end{bmatrix}^T
\end{align*}
$$



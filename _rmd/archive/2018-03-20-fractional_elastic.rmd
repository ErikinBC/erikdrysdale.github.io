---
title: "Fractional response models with elastic-net regularization"
output: html_document
---

$$
\newcommand{\bbeta}{\mathbf{\beta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\etab}{\mathbf{\eta}}
\newcommand{\bsigma}{\mathbf{\sigma}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bxi}{\bx_i}
\newcommand{\ei}{\varepsilon_i}
$$


## Overview

Binary classification is one the most studied problems in machine learning and many methods have been developed when the objective to predict between one of two distinct classes. However when a researcher wishes to model a response variable which contains fractional values, as well as possible exact binary values, there are fewer explicitely developed methods. Consider the case of a response variable which contains known binary states for some observations, and a probability assessment for others. In [mixture cure models](http://post.queensu.ca/~pengp/papers/Peng2014_4.pdf) (MCM), for example, the status of whether a patient will never experience the event (cured) or will eventually experience the event (not-cured) is known for some patients but not for others.[[^1]] In fact under the assumptions of the MCM, there is an exact relationship between the probability of being cured and the length of survival time. If these cure probabilities are combined with the known cure states, the response variable will contain both zeros, ones, and fractional values in between. 

The generalized linear model (GLM) approach to regression reparameterizes the natural parameters of the exponential distribution that is assumed to have generated the data so that the mean response is a function of a linear combination of features. The logistic regression parameterizes the mean as a linear combination of features within the logistic function.

$$
\begin{align*}
\eta_i &= \beta_0 + \beta_1 x_{i,1} + \dots + \beta_{p-1}x_{i,p-1} \\
&= \bxi^T \bbeta, \hspace{3mm} \bxi \in \mathbb{R}^p \\
E[y_i] &= p_i \in [0,1] \\
& =\frac{e^{\eta_i(\beta)}}{1+e^{\eta_i(\beta)}} \\
&= \sigma(\eta_i)
\end{align*}
$$

Can the logistic GLM be used in the case of fractional responses? The answer turns out to be yes. [Papke and Woolridge](https://faculty.smu.edu/millimet/classes/eco6375/papers/papke%20wooldridge%201996.pdf) showed that maximizing the Bernoulli log-likelihood gives a consistent estimate of $\bbeta$ because the estimate is a quasi-maximum likelihood estimator. Although special procedures would be needed to derive the standard errors. The Bernoilli log-likelihood turns out to be easy to maximize and allows for efficient regularization procedures along with the ability to weight observations. Furthermore the logistic function is a popular choice of $f : \mathbb{R} \to [0,1]$, $f(x)=1/(1+\exp(-x))$ due to it being infinitely differentiable, smooth, and having rotational symmetry. 

Formally, if we have $N$ tuples of observations $\{p_i,\bxi,w_i\}_{i=1}^N$, the (negative) log-likelihood we are trying to minimize is as follows where $w_i$ is the weight we would like to assign to each observation in our likelihood.[[^2]]

$$
\begin{align*}
\ell(\bbeta|\bp,\bw,\bx ) &= \sum_{i=1}^N w_i \Bigg\{ p_i (\bxi^T\bbeta) - \log(1+\exp(\bxi^T\bbeta)) \Bigg\} \\
\hat\bbeta &= \arg \min_{\bbeta} \hspace{3mm} - \ell(\bbeta)
\end{align*}
$$

The gradient is also easily calculable.

$$
\begin{align*}
&\textbf{Gradient} \\
\frac{\partial \ell(\bbeta|\bp,\bw,\bx)}{\partial \beta_j} &= \sum_{i=1}^N w_i x_{ij} \Big\{p_i - \sigma(\eta_i)  \Big\} \\
\frac{\partial \ell(\bbeta|\bp,\bw,\bx)}{\partial \beta} &= \bX^T \bW (\bp - \bsigma) \\
\bW &= \text{diag}\{w_1,\dots,w_N\}
\end{align*}
$$

<!-- &\textbf{Hessian} \\ -->
<!-- \frac{\partial ^2\ell(\bbeta|\bp,\bw,\bx)}{\partial \beta_j^2} &= - \sum_{i=1}^N w_i x_{ij}^2 \sigma(\eta_i)[1-\sigma(\eta_i)] \\ -->
<!-- \frac{\partial ^2\ell(\bbeta|\bp,\bw,\bx)}{\partial \bbeta \partial \bbeta^T} &= - \bX^T \bW \Omega \bX \\ -->
<!-- \Omega &= \text{diag}\{\sigma(\eta_1)[1-\sigma(\eta_1)],\dots,\sigma(\eta_N)[1-\sigma(\eta_N)]\} -->

## L1 & L2 normalization

The (negative) log-likelihood can be penalized so that parameter estimate will balance the bias/variance trade-off. The L1 (Lasso-type) or L2 (Ridge-type) regularization penalty term are a popular choice, where $\|\bbeta\|_1=\sum_{j=1}^p|\beta_j|$ and $\|\bbeta\|^2_2=\sum_{j=1}^p \beta_j^2$.

$$
\begin{align*}
&\textbf{Lasso-type} \\
\hat\bbeta &= \arg \min_{\bbeta} \hspace{3mm} - \ell(\bbeta) + \lambda \| \beta \|_1 = \ell_{\lambda_1}(\bbeta) \\
&\textbf{Ridge-type} \\
\hat\bbeta &= \arg \min_{\bbeta} \hspace{3mm} - \ell(\bbeta) + (\lambda/2) \| \beta \|_2 = \ell_{\lambda_2}(\bbeta) \\
\end{align*}
$$

For the L1 penality, we will have to take the subgradient because the L1 norm is non-differntiable at certain points. However the L2 penatly can have a traditionally defined gradient.

\ \ 

$$
\begin{align*}
&\textbf{Lasso-type subgradient} \\
\partial_{\beta_j} \ell_{\lambda_1}(\bbeta) &= -\bX^T_j \bW (\bp - \bsigma) +  \partial_{\beta_j} |\beta_j |  \\
&\textbf{Ridge-type subgradient} \\
\frac{\partial \ell_{\lambda_1}(\bbeta)}{\partial\beta_j} &= -\bX^T_j \bW (\bp - \bsigma) + \beta_j \\
\end{align*}
$$

Note that the subgradient of a function $g=\partial_x f(x)$, $x\in \mathbb{R}^n$, is any vector (also in $\mathbb{R}^n$) such that $f(y) \geq f(x) + g^T(y -x )$, which means that our subgradient underapproximates  our function about a first-order expansion (with at most an equality). In other words, all convex functions must have subgradients. Functions like the $\max$ operator or the absolute value are convex and therefore have subgradients. Subgradients also have the property that: $\partial_x f(x^*) \in 0, \hspace{2mm} x^* \in \arg \min_x f(x) $. In the case of the absolute value function $f(x)=|x|$ it is easy to see that: 

$$
\begin{align*}
|y| \geq |x| + g^T(y-x), \forall y \in \mathbb{R}^n \hspace{3mm} \longleftrightarrow 
g &= \begin{cases}
\text{sign}(x) & \text{ if } x\neq 0 \\
[-1,1] & \text{ otherwise}
\end{cases}
\end{align*}
$$

Suppose the function we are trying to minimize is: $f(x)=0.5(x-y)^2 + \lambda |x|$. We can calculate the subgradient, find the set of values in which it would be zero, and then consider the cases where $x^* \neq 0$.

$$
\begin{align*}
f(x) &= 0.5(x-y)^2 + \lambda |x| \\
\partial_x f(x) &= x - y + \lambda \cdot g \\
0 &\in x^* - y + \lambda \cdot g \\
&\text{If } x^* >0 \\
x^* &= y - \lambda > 0  \\
&\text{If } x^* <0 \\
x^* &= y + \lambda <- 0  \\
&\text{Therefore } x^*=0 \text{ if: } \\
(y - &\lambda < 0) \| (y + \lambda > 0) \longleftrightarrow \\
- &\lambda < -y < \lambda \longleftrightarrow \\
-&\lambda < y < \lambda
\end{align*}
$$

Hence the optimal solution for the function is the soft-thresholding function: $x^*=S_\lambda(y)=\max\{\text{sign}(y)(|y|-\lambda),0\}$.


## Footnotes

[^1]: In the case of cancer research, patients who have already died would be known to be not-cured and patients whose cancer is in complete remission can be considered cured (the latter is only true in certain situations).

[^2]: Because the log-likelihood requires minimizing the sum of $N$ functions, our weight values attributes the relative importance that the $i^{th}$ observation has in this minimization. 


geom_point(size=2)
ggplot(pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>% gather(key,val,-dataset),
aes(x=dataset,y=val,color=key)) +
geom_point(size=2) +
facet_wrap(~dataset,scales='free_y')
ggplot(pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>% gather(key,val,-dataset),
aes(x=key,y=val,color=key)) +
geom_point(size=2) +
facet_wrap(~dataset,scales='free_y')
pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>%
gather(key,val,-dataset)
pred.store[i,]
pred.store[i,-(1:4)]
names(pred.store[i,-(1:4)])
pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>%
gather(key,val,-dataset) %>% mutate(key=factor(key,levels=names(pred.store[i,-(1:4)])))
pred.long <- pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>%
gather(key,val,-dataset) %>% mutate(key=factor(key,levels=names(pred.store[i,-(1:4)])))
ggplot(pred.long,aes(x=key,y=val,color=key)) +
geom_point(size=2) +
facet_wrap(~dataset,scales='free_y')
temp.p
e <- rnorm(temp.p)
r2 <- sqrt(sum(e*e))
(t(X) %*% e)
(t(temp.X) %*% e)
temp.X %>% dim
i=0;nam=ds.nams[1]
i=i+1
print(sprintf('Iter: %i, dataset: %s',i,nam))
# Load the dataset
temp.fn <- str_c(ds.dir,str_c(nam,'.txt'),sep='/')
temp.dat <- fread(temp.fn)
temp.p <- ncol(temp.dat)
temp.N <- nrow(temp.dat)
# Data X ~ y
temp.X <- scale(as.matrix(temp.dat[,-(1:2)]))
# Remove any NAs
drop.idx <- apply(temp.X,2,function(cc) any(is.na(cc))) %>% which
if (length(drop.idx)>0) {
temp.X <- temp.X[,-drop.idx]
}
# Data.driven lambda
dim(temp.X)
t(temp.X) %*% e
dim(temp.X)
length(e)
temp.p
e <- rnorm(ncol(temp.X))
r2 <- sqrt(sum(e*e))
t(temp.X) %*% e
temp.X %*% e
dim(t(temp.X))
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
dim(t(temp.X) %*% e)
t(temp.X) %*% e
abs(t(temp.X) %*% e)
max(abs(t(temp.X) %*% e))
head(abs(t(temp.X) %*% e))
max(abs(t(temp.X) %*% e))
max(abs(t(temp.X) %*% e))/r2
sim.lam <- replicate(10,{
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
max(abs(t(temp.X) %*% e))/r2
})
sim.lam
sim.lam <- replicate(250,{
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
max(abs(t(temp.X) %*% e))/r2
})
sim.lam
quantile(sim.lam,0.95)
sqrt(log(p))
sqrt(log(temp.p))
i=i+1
print(sprintf('Iter: %i, dataset: %s',i,nam))
# Load the dataset
temp.fn <- str_c(ds.dir,str_c(nam,'.txt'),sep='/')
temp.dat <- fread(temp.fn)
temp.p <- ncol(temp.dat)
temp.N <- nrow(temp.dat)
# Data X ~ y
temp.X <- scale(as.matrix(temp.dat[,-(1:2)]))
# Remove any NAs
drop.idx <- apply(temp.X,2,function(cc) any(is.na(cc))) %>% which
if (length(drop.idx)>0) {
temp.X <- temp.X[,-drop.idx]
}
# Data.driven lambda
sim.lam <- replicate(250,{
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
max(abs(t(temp.X) %*% e))/r2
})
#
temp.y <- as.vector(scale(temp.dat$V2))
# temp.y <- as.factor(temp.dat$V1)
rm(temp.dat)
# # encode y and binary
# temp.y <- ifelse(temp.y==levels(temp.y)[1],1,0)
# idx.y0 <- which(temp.y==0)
# idx.y1 <- which(temp.y==1)
# Ratio
temp.yratio <- 1 #max(length(idx.y0),length(idx.y1))/min(length(idx.y0),length(idx.y1))
# Set seed for reproducibility
set.seed(1)
# # Index for training samples (conditional split)
# train.y0 <- sample(idx.y0,size=ceiling(length(idx.y0)*TRAIN),replace=F)
# train.y1 <- sample(idx.y1,size=ceiling(length(idx.y1)*TRAIN),replace=F)
train.idx <- sort(sample(1:length(temp.y),ceiling(length(temp.y)*TRAIN))) # sort(c(train.y0,train.y1))
test.idx <- setdiff(1:temp.N,train.idx)
# Assign datasets
train.X <- temp.X[train.idx,]
test.X <- temp.X[test.idx,]
train.y <- temp.y[train.idx]
test.y <- temp.y[test.idx]
rm(list=c('temp.X','temp.y'))
# --- Modelling --- #
# - (i) (Gaussian) SQRT-LASSO - #
# lam <- sqrt(log(temp.p)) #1.01 * qt(p=1-0.05/(2*temp.p),df=temp.N)
lam <- quantile(sim.lam,0.95)
sqrt.beta <- cd.sqrt.lasso(X=train.X,y=train.y,niter=10,lam=lam)$beta
idx.sqrt <- supp(sqrt.beta)[-1] -1
# Fit post models
mdl.post.gauss <- glm(train.y ~ train.X[,idx.sqrt],family=gaussian)
mdl.post.glmnet <- cv.glmnet(x=train.X[,idx.sqrt],y=train.y,alpha=0,nfolds=10)
# Make predictions
yhat.sqrt <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% sqrt.beta[supp(sqrt.beta)])
yhat.post.ols <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% coef(mdl.post.gauss))
yhat.post.ridge <- as.vector(cbind(1,test.X[,idx.sqrt]) %*%
as.vector(coef(mdl.post.glmnet,s='lambda.min')))
# - (ii) (Gaussian) LASSO-MIN + LASSO-1se - #
mdl.cv.gauss <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='gaussian')
# Make predictions
yhat.gauss.min <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.min'))
yhat.gauss.1se <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.1se'))
# - (iii) (Logistic) ST-LASSO - #
# cd.stlasso(X=temp.X[train.idx,],y=temp.y[train.idx],family='binomial',niter=10,lam=lam)$beta %>% supp
# # - (iv) (Logistic) LASSO-MIN + LASSO-1se - #
# mdl.cv.logistic <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='binomial')
# # Make predictions
# yhat.logistic.min <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.min'))
# yhat.logistic.1se <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.1se'))
# Put all predictions in a list
temp.list.yhat <- list(gauss.sqrt=yhat.sqrt,gauss.ols=yhat.post.ols,gauss.ridge=yhat.post.ridge,
gauss.min=yhat.gauss.min,gauss.1se=yhat.gauss.1se)
#logit.sqrt=yhat.post.logit,logit.min=yhat.logistic.min,logit.1se=yhat.logistic.1se
# Get accuracy
temp.acc <- lapply(temp.list.yhat,function(yhat) mean((test.y-yhat)^2) )
temp.acc
abs(t(temp.X) %*% e)/r2
i=0;nam=ds.nams[1]
i=i+1
print(sprintf('Iter: %i, dataset: %s',i,nam))
# Load the dataset
temp.fn <- str_c(ds.dir,str_c(nam,'.txt'),sep='/')
temp.dat <- fread(temp.fn)
temp.p <- ncol(temp.dat)
temp.N <- nrow(temp.dat)
# Data X ~ y
temp.X <- scale(as.matrix(temp.dat[,-(1:2)]))
# Remove any NAs
drop.idx <- apply(temp.X,2,function(cc) any(is.na(cc))) %>% which
if (length(drop.idx)>0) {
temp.X <- temp.X[,-drop.idx]
}
# Data.driven lambda
sim.lam <- replicate(250,{
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
abs(t(temp.X) %*% e)/r2
})
sim.lam %>% hist
quantile(sim.lam,.95)
lam <- quantile(sim.lam,.95)
sqrt.beta <- cd.sqrt.lasso(X=train.X,y=train.y,niter=10,lam=lam)$beta
idx.sqrt <- supp(sqrt.beta)[-1] -1
mdl.post.gauss <- glm(train.y ~ train.X[,idx.sqrt],family=gaussian)
mdl.post.glmnet <- cv.glmnet(x=train.X[,idx.sqrt],y=train.y,alpha=0,nfolds=10)
# Make predictions
yhat.sqrt <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% sqrt.beta[supp(sqrt.beta)])
yhat.post.ols <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% coef(mdl.post.gauss))
yhat.post.ridge <- as.vector(cbind(1,test.X[,idx.sqrt]) %*%
as.vector(coef(mdl.post.glmnet,s='lambda.min')))
# - (ii) (Gaussian) LASSO-MIN + LASSO-1se - #
mdl.cv.gauss <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='gaussian')
# Make predictions
yhat.gauss.min <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.min'))
yhat.gauss.1se <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.1se'))
# - (iii) (Logistic) ST-LASSO - #
# cd.stlasso(X=temp.X[train.idx,],y=temp.y[train.idx],family='binomial',niter=10,lam=lam)$beta %>% supp
# # - (iv) (Logistic) LASSO-MIN + LASSO-1se - #
# mdl.cv.logistic <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='binomial')
# # Make predictions
# yhat.logistic.min <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.min'))
# yhat.logistic.1se <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.1se'))
# Put all predictions in a list
temp.list.yhat <- list(gauss.sqrt=yhat.sqrt,gauss.ols=yhat.post.ols,gauss.ridge=yhat.post.ridge,
gauss.min=yhat.gauss.min,gauss.1se=yhat.gauss.1se)
#logit.sqrt=yhat.post.logit,logit.min=yhat.logistic.min,logit.1se=yhat.logistic.1se
# Get accuracy
temp.acc <- lapply(temp.list.yhat,function(yhat) mean((test.y-yhat)^2) )
temp.acc
cn <- c('dataset','N','p','yratio',
'gauss.sqrt','gauss.ols','gauss.ridge','gauss.min','gauss.1se')
# 'logit.sqrt','logit.min','logit.1se')
pred.store <- data.frame(matrix(NA,nrow=length(ds.nams),ncol=length(cn)))
colnames(pred.store) <- cn
pred.store$dataset <- ds.nams
# Define the training/test split
TRAIN <- 0.75
# supp fun
supp <- function(x) { which(x!=0) }
expit <- function(x) { 1/(1+exp(-x)) }
i=0;nam=ds.nams[1]
for (nam in ds.nams) {
i=i+1
print(sprintf('Iter: %i, dataset: %s',i,nam))
# Load the dataset
temp.fn <- str_c(ds.dir,str_c(nam,'.txt'),sep='/')
temp.dat <- fread(temp.fn)
temp.p <- ncol(temp.dat)
temp.N <- nrow(temp.dat)
# Data X ~ y
temp.X <- scale(as.matrix(temp.dat[,-(1:2)]))
# Remove any NAs
drop.idx <- apply(temp.X,2,function(cc) any(is.na(cc))) %>% which
if (length(drop.idx)>0) {
temp.X <- temp.X[,-drop.idx]
}
# Data.driven lambda
sim.lam <- replicate(250,{
e <- rnorm(nrow(temp.X))
r2 <- sqrt(sum(e*e))
abs(t(temp.X) %*% e)/r2
})
#
temp.y <- as.vector(scale(temp.dat$V2))
# temp.y <- as.factor(temp.dat$V1)
rm(temp.dat)
# # encode y and binary
# temp.y <- ifelse(temp.y==levels(temp.y)[1],1,0)
# idx.y0 <- which(temp.y==0)
# idx.y1 <- which(temp.y==1)
# Ratio
temp.yratio <- 1 #max(length(idx.y0),length(idx.y1))/min(length(idx.y0),length(idx.y1))
# Set seed for reproducibility
set.seed(1)
# # Index for training samples (conditional split)
# train.y0 <- sample(idx.y0,size=ceiling(length(idx.y0)*TRAIN),replace=F)
# train.y1 <- sample(idx.y1,size=ceiling(length(idx.y1)*TRAIN),replace=F)
train.idx <- sort(sample(1:length(temp.y),ceiling(length(temp.y)*TRAIN))) # sort(c(train.y0,train.y1))
test.idx <- setdiff(1:temp.N,train.idx)
# Assign datasets
train.X <- temp.X[train.idx,]
test.X <- temp.X[test.idx,]
train.y <- temp.y[train.idx]
test.y <- temp.y[test.idx]
rm(list=c('temp.X','temp.y'))
# --- Modelling --- #
# - (i) (Gaussian) SQRT-LASSO - #
# lam <- sqrt(log(temp.p)) #1.01 * qt(p=1-0.05/(2*temp.p),df=temp.N)
lam <- quantile(sim.lam,.95)
sqrt.beta <- cd.sqrt.lasso(X=train.X,y=train.y,niter=10,lam=lam)$beta
idx.sqrt <- supp(sqrt.beta)[-1] -1
# Fit post models
mdl.post.gauss <- glm(train.y ~ train.X[,idx.sqrt],family=gaussian)
mdl.post.glmnet <- cv.glmnet(x=train.X[,idx.sqrt],y=train.y,alpha=0,nfolds=10)
# Make predictions
yhat.sqrt <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% sqrt.beta[supp(sqrt.beta)])
yhat.post.ols <- as.vector(cbind(1,test.X[,idx.sqrt]) %*% coef(mdl.post.gauss))
yhat.post.ridge <- as.vector(cbind(1,test.X[,idx.sqrt]) %*%
as.vector(coef(mdl.post.glmnet,s='lambda.min')))
# - (ii) (Gaussian) LASSO-MIN + LASSO-1se - #
mdl.cv.gauss <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='gaussian')
# Make predictions
yhat.gauss.min <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.min'))
yhat.gauss.1se <- as.vector(predict.cv.glmnet(mdl.cv.gauss,newx=test.X,s='lambda.1se'))
# - (iii) (Logistic) ST-LASSO - #
# cd.stlasso(X=temp.X[train.idx,],y=temp.y[train.idx],family='binomial',niter=10,lam=lam)$beta %>% supp
# # - (iv) (Logistic) LASSO-MIN + LASSO-1se - #
# mdl.cv.logistic <- cv.glmnet(x=train.X,y=train.y,nfolds=10,family='binomial')
# # Make predictions
# yhat.logistic.min <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.min'))
# yhat.logistic.1se <- as.vector(predict.cv.glmnet(mdl.cv.logistic,
#                                                  newx=test.X,s='lambda.1se'))
# Put all predictions in a list
temp.list.yhat <- list(gauss.sqrt=yhat.sqrt,gauss.ols=yhat.post.ols,gauss.ridge=yhat.post.ridge,
gauss.min=yhat.gauss.min,gauss.1se=yhat.gauss.1se)
#logit.sqrt=yhat.post.logit,logit.min=yhat.logistic.min,logit.1se=yhat.logistic.1se
# Get accuracy
temp.acc <- lapply(temp.list.yhat,function(yhat) mean((test.y-yhat)^2) )
# temp.acc <- lapply(temp.list.yhat,function(yhat) mean(test.y==ifelse(yhat>0.5,1,0)) )
# Store
pred.store[i,names(unlist(temp.acc))] <- unlist(temp.acc)
pred.store[i,c('N','p','yratio')] <- c(temp.N,temp.p,temp.yratio)
}
pred.long <- pred.store %>% tbl_df %>% dplyr::select(-c(N:yratio)) %>%
gather(key,val,-dataset) %>% mutate(key=factor(key,levels=names(pred.store[i,-(1:4)])))
ggplot(pred.long,aes(x=key,y=val,color=key)) +
geom_point(size=2) +
facet_wrap(~dataset,scales='free_y')
pred.long
ggplot(filter(pred.long,key!='gauss.ols'),aes(x=key,y=val,color=key)) +
geom_point(size=2) +
facet_wrap(~dataset,scales='free_y')
dev.off()
dir.base <- 'C:\Users\erikinwest\Dropbox\School\Project\ipilimumab_paper\code'
dir.base <- 'C:/Users/erikinwest/Dropbox/School/Project/ipilimumab_paper/code'
setwd(dir.base)
dir.data <- 'C:/Users/erikinwest/Documents/Research/ices/transfers'
paste(dir.data,'te_estimate.txt',sep='/')
read.table(paste(dir.data,'te_estimate.txt',sep='/'))
list.files(dir.data)
read.table(paste(dir.data,'te_estimate.txt',sep='/'))
read.table(paste(dir.data,'te_estimate.txt',sep='/'))
readLines(paste(dir.data,'te_estimate.txt',sep='/'))
qq <- readLines(paste(dir.data,'te_estimate.txt',sep='/'))
qq[1:2]
library(stringr);library(magrittr)
sapply(qq[1:2],function(rr) str_trim(rr))
sapply(qq[1:2],function(rr) str_split(str_trim(rr),' '))
qq[1]
str_split(qq[1],' ')
str_split(qq[1:2],' ')
lapply(str_split(qq[1:2],' '),function(ll) which(str_length(ll)>0))
lapply(str_split(qq[1:2],' '),function(ll) ll[which(str_length(ll)>0)])
qq <- readLines(paste(dir.data,'te_estimate.txt',sep='/'))
lapply(str_split(qq[1:2],' '),function(ll) ll[which(str_length(ll)>0)])
lapply(str_split(qq[1:2],' '),function(ll) ll[which(str_length(ll)>0)]) %>%
do.call('rbind',.)
lapply(str_split(qq[1:2],' '),function(ll) ll[which(str_length(ll)>0)]) %>%
do.call('rbind',.) %>% data.frame
dat1 <- lapply(str_split(qq,' '),function(ll) ll[which(str_length(ll)>0)]) %>%
do.call('rbind',.) %>% data.frame
data.frame(dat1[-1,])
dat2 <- data.frame(dat1[-1,])
setcolnames(dat2) <- dat1[1,]
colnames(dat2) <- dat1[1,]
dat2 %>% head
dat1[1,]
unlist(dat1[1,])
as.character(dat1[1,])
dat1[1,]
t(dat1[1,])
t(dat1[1,])[,1]
as.character(t(dat1[1,])[,1])
colnames(dat2) <- as.character(t(dat1[1,])[,1])
dat2 %>% head
dat2 <- dat2[,-1]
library(cowplot)
sapply(dat2,class)
head(dat2)
apply(dat2[,3:5],2,function(cc) as.numeric(as.character(cc)))
dat2[,3:5] <- apply(dat2[,3:5],2,function(cc) as.numeric(as.character(cc)))
head(dat2)
pd <- position_dodge()
ggplot(dat2,aes(x=dataset,y=value,color=type)) +
geom_point(size=2,position=pd) +
geom_linerange(aes(ymin=`lower.95`,ymax=`upper.95`),position=pd,linetype=2) +
facet_wrap(~time,nrow=1)
pd <- position_dodge(0.3)
ggplot(dat2,aes(x=dataset,y=value,color=type)) +
geom_point(size=2,position=pd) +
geom_linerange(aes(ymin=`lower.95`,ymax=`upper.95`),position=pd,linetype=2) +
facet_wrap(~time,nrow=1)
ggplot(dat2,aes(x=dataset,y=value,color=type,shape=variable)) +
geom_point(size=2,position=pd) +
geom_linerange(aes(ymin=`lower.95`,ymax=`upper.95`),position=pd,linetype=2) +
facet_wrap(~time,nrow=1)
dat2$variable
dat2$variable <- factor(dat2$variable,levels=c('nrob','rob'),labels=c('DS','rDS'))
dat2$type
dat2$type <- factor(dat2$type,levels=c('low','medium','high'))
dat2$dataset
ggplot(dat2,aes(x=dataset,y=value,color=type,shape=variable)) +
geom_point(size=2,position=pd) +
geom_linerange(aes(ymin=`lower.95`,ymax=`upper.95`),position=pd,linetype=2) +
facet_wrap(~time,nrow=1)
dat2 <- data.frame(dat1[-1,])
colnames(dat2) <- as.character(t(dat1[1,])[,1])
dat2 <- dat2[,-1]
dat2[,3:5] <- apply(dat2[,3:5],2,function(cc) as.numeric(as.character(cc)))
dat2$variable <- factor(dat2$variable,levels=c('nrob','rob'),labels=c('DS','rDS'))
dat2$type
dat2$type <- factor(dat2$type,levels=c('low','med','high'))
ggplot(dat2,aes(x=dataset,y=value,color=type,shape=variable)) +
geom_point(size=2,position=pd) +
geom_linerange(aes(ymin=`lower.95`,ymax=`upper.95`),position=pd,linetype=2) +
facet_wrap(~time,nrow=1)
rm(list=ls())
# Libraries
ll <- c('dplyr','magrittr','reshape2','stringr','ggplot2','survival','glmnet',
'stargazer','xtable','MatchIt','gtools')
sapply(ll,library,character.only=T)
#######################################################################
############# ------ STEP 1: LOAD DESIGN MATRICES ------ ##############
#######################################################################
data.dir <- '/linux_home/edrysdale/Files/projects/cancer/p0800.181.001/level3/ErikDrysdale/'
setwd(data.dir)
load('design_matrices.RData')
erik.only <- F
erik.only <- T
# Use the cancer dataset
cancer2 <- apply(survival::cancer,2,function(cc) ifelse(is.na(cc),
sample(cc[!is.na(cc)],sum(is.na(cc))),cc)) %>% tbl_df %>%
dplyr::rename(ipi=sex) %>% mutate(ipi=ifelse(ipi==2,1,0))
# Assign the code
cancer2$ikn <- paste('id',1:nrow(cancer2),sep='')
# Recode the institions
library(forcats)
cancer2$inst <- fct_lump(as.factor(cancer2$inst),n=9)
# Get the design matrix
X.cancer2 <- model.matrix(~ipi+age+ph.ecog+ph.karno+meal.cal+wt.loss+factor(inst),data=cancer2)[,-1]
# Add on some Gaussian white noise
# Time-invariant
dmat.unscale <- data.frame(ikn=cancer2$ikn,time=cancer2$time,
event=ifelse(cancer2$status==2,1,0),X.cancer2)
dmat.scale <- dmat.unscale
dmat.scale[,5:9] <- scale(dmat.unscale[,5:9])
# Time-dependent
dmat.td.unscale <- cbind(dmat.unscale[,1:2],time2=dmat.unscale$time+1,dmat.unscale[,-(1:2)])
dmat.td.scale <- cbind(dmat.unscale[,1:2],time2=dmat.scale$time+1,dmat.scale[,-(1:2)])
# Create the idx.tbl
set.seed(1)
var.ipi1 <- sample(x=c(1,0,NA),size=nrow(dmat.scale),prob=c(0.5,0.25,0.25),replace = T)
idx.tbl <- data.frame(ikn=dmat.scale$ikn,ipi_vs_dac_short=var.ipi1,ipi=dmat.scale$ipi)
head2 <- function(x) {print(x[1:6,1:6])}
# Put all the data sets in a list
dmat.list <- list(ti=list(scale=cbind(dmat.scale[,1:2],time2=dmat.scale$time,dmat.scale[,-(1:2)]),
unscale=cbind(dmat.unscale[,1:2],time2=dmat.unscale$time,dmat.unscale[,-(1:2)])),
td=list(scale=dmat.td.scale,unscale=dmat.td.unscale))
# --- GET THREE DIFFERENT RESPONSE CUTS --- #
# Get the IKN's for each of the three indexes
# (i) Dacarbazine + 1st line + 2nd line
all.ikn <- idx.tbl$ikn
# (ii) Dacarbazine + 1st line
first.ikn <- idx.tbl %>% filter(!is.na(ipi_vs_dac_short)) %>%  use_series(ikn)
# (ii) Dacarbazine + 2nd line
second.ikn <- idx.tbl %>% filter(is.na(ipi_vs_dac_short) | ipi_vs_dac_short==0) %>%  use_series(ikn)
# Store as a list
ikn.list <- list(all=all.ikn,first=first.ikn,second=second.ikn)
if (erik.only) {
ikn.list <- list(all=all.ikn,first=all.ikn,second=all.ikn)
}
# --- TRANSFORM EACH VARIABLE FOR TIME --- #
# Add on a time dependent dataset for each
dmat.trans <- lapply(dmat.list,function(ll) lapply(ll,function(qq)
mutate_at(qq,vars(-one_of(c('ikn','time','time2','event','ipi'))),funs(.*1))))
#dplyr::select(-matches('\\_drop'))))
# For each dataset, create the three partitions
dmat.trans <-
lapply(dmat.trans,function(ll) lapply(ll,function(qq)
mapply(function(idx) filter(qq,ikn %in% idx),ikn.list,SIMPLIFY = F) ))
So.trans <- list(ti=lapply(dmat.trans$ti,function(l1) lapply(l1,function(l2)
Surv(time=l2$time,event=l2$event))),
td=lapply(dmat.trans$td,function(l1) lapply(l1,function(l2)
Surv(time=l2$time,time2=l2$time2,event=l2$event))))
dmat.trans
dmat.trans %>% dim
dmat.trans$ti %>% head
dmat.trans$ti %>% dim
dmat.trans$ti$scale %>% head
dmat.trans$ti$scale %>% class
dmat.trans$td$scale$all %>% head
dmat.trans$td$scale$all[,1:5]
lapply(dmat.trans$td,function(l1) lapply(l1,function(l2) head(l2[,1:5])))
lapply(dmat.trans$td,function(l1) lapply(l1,function(l2) head(l2[,1:5])))
dev.off()
quantile(abs(rnorm(100)),0.95)
# Limits printing output
options(max.print = 500)
# Load in CRAN packages
ll.cran <- c('tidyverse','stringr','forcats','cowplot','data.table','glmnet','datamicroarray')
for (k in ll.cran) {library(k,character.only=T) }
# Directories
dir.base <- 'C:/Users/erikinwest/Documents/blogs/github/erikdrysdale.github.io/_rmd/extra_sqrt/'
setwd(dir.base)
rm(list=ls())
# Call in our algorithms
source('cdfuns.R')
# Get the dataset names and folder
ds.nams <- describe_data() %>% pull(author) %>% as.character
ds.dir <- 'C:/Users/erikinwest/Documents/Research/BL/datasets'
cn <- c('dataset','N','p','yratio',
'gauss.sqrt','gauss.ols','gauss.ridge','gauss.min','gauss.1se')
# 'logit.sqrt','logit.min','logit.1se')
pred.store <- data.frame(matrix(NA,nrow=length(ds.nams),ncol=length(cn)))
colnames(pred.store) <- cn
pred.store$dataset <- ds.nams
# Define the training/test split
TRAIN <- 0.75
# supp fun
supp <- function(x) { which(x!=0) }
expit <- function(x) { 1/(1+exp(-x)) }
i=0;nam=ds.nams[1]
print(sprintf('Iter: %i, dataset: %s',i,nam))
# Load the dataset
temp.fn <- str_c(ds.dir,str_c(nam,'.txt'),sep='/')
temp.dat <- fread(temp.fn)
temp.p <- ncol(temp.dat)
temp.N <- nrow(temp.dat)
# Data X ~ y
temp.X <- scale(as.matrix(temp.dat[,-(1:2)]))

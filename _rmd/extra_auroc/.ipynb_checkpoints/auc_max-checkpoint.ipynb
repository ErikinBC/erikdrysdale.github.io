{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxX5mm9eYrI3"
   },
   "source": [
    "# Direct AUROC optimization with PyTorch\n",
    "\n",
    "$$\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\beta}{\\boldsymbol{\\eta}}\n",
    "\\newcommand{\\bw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "$$\n",
    "\n",
    "In this post I'll discuss how to directly optimize the Area Under the Receiver Operating Characteristic ([AUROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), which measures the discriminatory ability of a model across a range of sensitivity/specicificity thresholds for binary classification. The AUROC is often used as method to benchmark different models and has the added benefit that its properties are independent of the underlying class imbalance. \n",
    "\n",
    "The AUROC is a specific instance of the more general [learning to rank](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) class of problems as the AUROC is the proportion of scores from a positive class that exceed the scores from a negative class. More formally if the outcome for the $i^{th}$ observation is $y \\in \\{0,1\\}$, and has a corresponding risk score $\\eta_i$, then the AUROC for $\\by$ and $\\beta$ will be:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{AUROC}(\\by,\\beta) &= \\frac{1}{|I_1|\\cdot|I_0|} \\sum_{i \\in I_1} \\sum_{j \\in I_0} \\Big[ I[\\eta_i > \\eta_j] + 0.5I[\\eta_i = \\eta_j] \\Big] \\\\\n",
    "I_k &= \\{i: y_i = k \\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Most AUROC formulas grant a half-point for tied scores. As has been [discussed before](http://www.erikdrysdale.com/survConcordance/), optimizing indicator functions $I(\\cdot)$ is NP-hard, so instead a convex relation of the AUROC can be calculated.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{cAUROC}(\\by,\\beta) &= \\frac{1}{|I_1|\\cdot|I_0|} \\sum_{i \\in I_1} \\sum_{j \\in I_0} \\log \\sigma [\\eta_i - \\eta_j]  \\\\\n",
    "\\sigma(z) &= \\frac{1}{1+\\exp(-z)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The cAUROC formula encorages the log-odds of the positive class ($y=0$) to be as large as possible with respect to the negative class ($y=0$).\n",
    "\n",
    "## (1) Optimization with linear methods\n",
    "\n",
    "Before looking at a neural network method, this first section will show how to directly optimize the cAUROC with a linear combination of features. We'll compare this approach to the standard logistic regression method and see if there is a meaningful difference. If we encode $\\eta_i = \\bx_i^T\\bw$, and apply the chain rule we can see that the derivative for the cAUROC will be:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{cAUROC}(\\by,\\beta)}{\\partial \\bw} &= \\frac{1}{|I_1|\\cdot|I_0|} \\sum_{i \\in I_1} \\sum_{j \\in I_0} (1 -  \\sigma [\\eta_i - \\eta_j] ) [\\bx_i - \\bx_j]  \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WcmAAeyaYp1m"
   },
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def sigmoid(x):\n",
    "  return( 1 / (1 + np.exp(-x)) )\n",
    "\n",
    "def idx_I0I1(y):\n",
    "  return( (np.where(y == 0)[0], np.where(y == 1)[0] ) )\n",
    "\n",
    "def AUROC(eta,idx0,idx1):\n",
    "  den = len(idx0) * len(idx1)\n",
    "  num = 0\n",
    "  for i in idx1:\n",
    "    num += sum( eta[i] > eta[idx0] ) + 0.5*sum(eta[i] == eta[idx0])\n",
    "  return(num / den)\n",
    "\n",
    "def cAUROC(w,X,idx0,idx1):\n",
    "  eta = X.dot(w)\n",
    "  den = len(idx0) * len(idx1)\n",
    "  num = 0\n",
    "  for i in idx1:\n",
    "    num += sum( np.log(sigmoid(eta[i] - eta[idx0])) )\n",
    "  return( - num / den)\n",
    "\n",
    "def dcAUROC(w, X, idx0, idx1):\n",
    "  eta = X.dot(w)\n",
    "  n0, n1 =  len(idx0), len(idx1)\n",
    "  den = n0 * n1\n",
    "  num = 0\n",
    "  for i in idx1:\n",
    "    num += ((1 - sigmoid(eta[i] - eta[idx0])).reshape([n0,1]) * (X[[i]] - X[idx0]) ).sum(axis=0) # *\n",
    "  return( - num / den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVGNcaDYZIeU"
   },
   "source": [
    "In the example simulations below the [Boston dataset](https://scikit-learn.org/stable/datasets/index.html#boston-dataset) will be used where the binary outcome is whether a house price is in the 90th percentile or higher (i.e. the top 10% of prices in the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnJj7ja4qOR5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = load_boston(return_X_y=True)\n",
    "# binarize\n",
    "y = np.where(y > np.quantile(y,0.9), 1 , 0)\n",
    "\n",
    "nsim = 100\n",
    "holder_auc = []\n",
    "holder_w = []\n",
    "winit = np.repeat(0,X.shape[1])\n",
    "for kk in range(nsim):\n",
    "  y_train, y_test, X_train, X_test = train_test_split(y, X, test_size=0.2, random_state=kk, stratify=y)\n",
    "  enc = StandardScaler().fit(X_train)\n",
    "  idx0_train, idx1_train = idx_I0I1(y_train)\n",
    "  idx0_test, idx1_test = idx_I0I1(y_test)\n",
    "  w_auc = minimize(fun=cAUROC,x0=winit,\n",
    "                  args=(enc.transform(X_train), idx0_train, idx1_train),\n",
    "                  method='L-BFGS-B',jac=dcAUROC).x\n",
    "  eta_auc = enc.transform(X_test).dot(w_auc)\n",
    "  mdl_logit = LogisticRegression(penalty='none')\n",
    "  eta_logit = mdl_logit.fit(enc.transform(X_train),y_train).predict_proba(X_test)[:,1]\n",
    "  auc1, auc2 = roc_auc_score(y_test,eta_auc), roc_auc_score(y_test,eta_logit)\n",
    "  holder_auc.append([auc1, auc2])\n",
    "  holder_w.append(pd.DataFrame({'cn':load_boston()['feature_names'],'auc':w_auc,'logit':mdl_logit.coef_.flatten()}))\n",
    "\n",
    "auc_mu = np.vstack(holder_auc).mean(axis=0)\n",
    "print('AUC from cAUROC: %0.2f%%\\nAUC for LogisticRegression: %0.2f%%' % \n",
    "      (auc_mu[0], auc_mu[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCI8Q1q-UYNz"
   },
   "source": [
    "We can see that the AUC minimizer finds a linear combination of features that is has a significantly higher AUC. Because the logistic regression uses a simple logistic loss function, the model has an incentive in prioritizing predicting low probabilities because most of the labels are zero. In contrast, the AUC minimizer is independent of this class balance. \n",
    "\n",
    "The figure below shows while the coefficients between the AUC model are highly correlated, their slight differences account for the meaningful performance gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2GxqYW1UuXu"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df_w = pd.concat(holder_w) #.groupby('cn').mean().reset_index()\n",
    "g = sns.FacetGrid(data=df_w,col='cn',col_wrap=5,hue='cn',sharex=False,sharey=False)\n",
    "g.map(plt.scatter, 'logit','auc')\n",
    "g.set_xlabels('Logistic coefficients')\n",
    "g.set_ylabels('cAUROC coefficients')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Figure: Comparison of LR and cAUROC cofficients per simulation',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nof9sr7GYR1D"
   },
   "source": [
    "## (2) AUC minimization with PyTorch\n",
    "\n",
    "To optimize a neural network in PyTorch with the goal of minimizing the cAUROC we will draw a given $i,j$ pair where $i \\in I_1$ and $j \\in I_0$. While other mini-batch approaches are possible (including the full-batch approach used for the gradient functions above), this mini-batch of two method will have the smallest memory overhead. The stochastic gradient for our network $f_\\theta$ will now be:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Bigg[\\frac{\\partial f_\\theta}{\\partial \\theta}\\Bigg]_{i,j} &= \\frac{\\partial}{\\partial \\theta} \\log \\sigma [ f_\\theta(\\bx_i) - f_\\theta(\\bx_j) ]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where $f(\\cdot)$ is 1-dimensional neural network output and $\\theta$ are the network parameters. The gradient of this deep neural network will be calculated by PyTorch's automatic differention backend.\n",
    "\n",
    "The example dataset will be the [California](https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset) housing price dataset. To make the prediction task challenging, house prices will first be partially scrambled with noise, and then outcome will binarize by labelling only the top 5% of housing prices as the positive class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zit9bB5_YpNO"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "np.random.seed(1234)\n",
    "data = fetch_california_housing(download_if_missing=True)\n",
    "cn_cali = data.feature_names\n",
    "X_cali = data.data\n",
    "y_cali = data.target\n",
    "y_cali += np.random.randn(y_cali.shape[0])*(y_cali.std())\n",
    "y_cali = np.where(y_cali > np.quantile(y_cali,0.95),1,0)\n",
    "y_cali_train, y_cali_test, X_cali_train, X_cali_test = \\\n",
    "  train_test_split(y_cali, X_cali, test_size=0.2, random_state=1234, stratify=y_cali)\n",
    "enc = StandardScaler().fit(X_cali_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "809asQHxhz-g"
   },
   "source": [
    "In the next code block below, we will define the neural network class, the optimizer, and the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stF6zn3KVnC8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ffnet(nn.Module):\n",
    "    def __init__(self,num_features):\n",
    "      super(ffnet, self).__init__()\n",
    "      p = num_features\n",
    "      self.fc1 = nn.Linear(p, 36)\n",
    "      self.fc2 = nn.Linear(36, 12)\n",
    "      self.fc3 = nn.Linear(12, 6)\n",
    "      self.fc4 = nn.Linear(6,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = F.relu(self.fc3(x))\n",
    "      x = self.fc4(x)\n",
    "      return(x)\n",
    "\n",
    "# Binary loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Seed the network\n",
    "torch.manual_seed(1234)\n",
    "nnet = ffnet(num_features=X_cali.shape[1])\n",
    "optimizer = torch.optim.Adam(params=nnet.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvneOezbb2o9"
   },
   "source": [
    "In the next code block, we'll set up the sampling strategy and train the network until the AUC on the validation set exceeds 90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISRsbtG1V84y"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "y_cali_R, y_cali_V, X_cali_R, X_cali_V = \\\n",
    "  train_test_split(y_cali_train, X_cali_train, test_size=0.2, random_state=1234, stratify=y_cali_train)\n",
    "enc = StandardScaler().fit(X_cali_R)\n",
    "\n",
    "idx0_R, idx1_R = idx_I0I1(y_cali_R)\n",
    "\n",
    "nepochs = 100\n",
    "\n",
    "auc_holder = []\n",
    "for kk in range(nepochs):\n",
    "  print('Epoch %i of %i' % (kk+1, nepochs))\n",
    "  # Sample class 0 pairs\n",
    "  idx0_kk = np.random.choice(idx0_R,len(idx1_R),replace=False) \n",
    "  for i,j in zip(idx1_R, idx0_kk):\n",
    "    optimizer.zero_grad() # clear gradient\n",
    "    dlogit = nnet(torch.Tensor(enc.transform(X_cali_R[[i]]))) - \\\n",
    "        nnet(torch.Tensor(enc.transform(X_cali_R[[j]]))) # calculate log-odd differences\n",
    "    loss = criterion(dlogit.flatten(), torch.Tensor([1]))\n",
    "    loss.backward() # backprop\n",
    "    optimizer.step() # gradient-step\n",
    "  # Calculate AUC on held-out validation\n",
    "  auc_k = roc_auc_score(y_cali_V,\n",
    "    nnet(torch.Tensor(enc.transform(X_cali_V))).detach().flatten().numpy())\n",
    "  if auc_k > 0.9:\n",
    "    print('AUC > 90% achieved')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RsV-c_tpODA"
   },
   "outputs": [],
   "source": [
    "# Compare performance on final test set\n",
    "auc_nnet_cali = roc_auc_score(y_cali_test,\n",
    "    nnet(torch.Tensor(enc.transform(X_cali_test))).detach().flatten().numpy())\n",
    "\n",
    "# Fit a benchmark model\n",
    "logit_cali = LogisticRegression(penalty='none',solver='lbfgs',max_iter=1000)\n",
    "logit_cali.fit(enc.transform(X_cali_train), y_cali_train)\n",
    "auc_logit_cali = roc_auc_score(y_cali_test,logit_cali.predict_proba(enc.transform(X_cali_test))[:,1])\n",
    "\n",
    "print('nnet-AUC: %0.3f, logit: %0.3f' % (auc_nnet_cali, auc_logit_cali))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPgJNJ4YnRl7"
   },
   "source": [
    "While the performance gains turned out the minimal in this case (an extra 2% AUC), the writing the optimizer was exceptional easy to do in PyTorch and converged in under a dozen epochs. This architecture lends itself to any other learn-to-rank architecture."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "auc_max.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

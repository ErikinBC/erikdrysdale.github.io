
This helps to ensure that the chosen $\lambda$ does not unnessarily shrink the coefficients simply because they are only found in a subset of the loss functions, helping to get a more accurate global fit. The more general formulation of the KKT condition will be:

$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta_j} &= \frac{1}{N} \sum_{d \in S_j} \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_j} l^{(d)}(\beta^{(d)})  \Bigg\} + \Bigg( \sum_{d \in S_j}  \pi^{(d)}\Bigg) \Bigg[ (1-\alpha) \lambda \beta_j + \alpha \lambda \partial_{\beta_j} |\beta_j |  \Bigg] \\
&\text{Solving this equation w.r.t zero allows us to divide both sides:} \\
\frac{\partial l_M(\beta)}{\partial \beta_j} &= \frac{1}{N} \Bigg( \sum_{d \in S_j}  \pi^{(d)}\Bigg)^{-1} \sum_{d \in S_j} \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_j} l^{(d)}(\beta^{(d)})  \Bigg\} + (1-\alpha) \lambda \beta_j + \alpha \lambda g_j \\
&= \frac{1}{N} J_{\beta,j} + P_{\beta,j} \\
\nabla l_M(\beta) &= \frac{1}{N} J_{\beta} + P_{\beta}
\end{align*}
$$

Where $g_j$ is the subgradient for the absolute value function. To vectorize this, we'll need to introduce some additional notation:

$$
\begin{align*}
&\text{For the likelihood term} \\
\\
l(\beta) &= [l^{(1)}(\beta^{(1)}),\dots,l^{(D)}(\beta^{(D)})] \in \mathbb{R}^{1 \times D} \\
\\
\frac{\partial l(\beta)}{\partial \beta} &= \nabla_{\beta} l(\beta) = \begin{bmatrix} 
\frac{\partial l^{(1)}(\beta^{(1)})}{\partial \beta_1} & \cdots & \frac{\partial l^{(D)}(\beta^{(D)})}{\partial \beta_1} \\
\vdots & & \vdots \\
\frac{\partial l^{(1)}(\beta^{(1)})}{\partial \beta_p} & \cdots & \frac{\partial l^{(D)}(\beta^{(D)})}{\partial \beta_p}
\end{bmatrix} \in \mathbb{R}^{p \times D} \\
\\
[S]_{i,j} &= I(j \in S_i), \hspace{2mm} S \in \{0,1 \}^{p\times D} \\
\\
\Pi &= \iota_p \otimes \pi^T \in [0,1]^{p \times D}, \hspace{2mm} \pi = [\pi^{(1)},\dots,\pi^{(D)}]^T \\
\\
\Omega &= (S \circ \Pi) \iota_D \in [0,1]^{p \times 1} \\
\\
J_\beta &= \Omega^{-1} \circ \big(\nabla_\beta l(\beta) \circ S \circ \Pi \big) \iota_D \\
\\
&\text{For the penalized term} \\
P_\beta &= (1-\alpha)\lambda\beta + \alpha\lambda g
\end{align*}
$$

Using our specific example in the beginning:

$$
\begin{align*}
S_1 &= \{1,2 \}, S_2 = \{1,2\}, S_3 = \{1\} \\
\nabla_\beta l(\beta) &= \begin{pmatrix}
\nabla_1^{(1)} & \nabla_1^{(2)}  \\
\nabla_2^{(1)} & \nabla_2^{(2)} \\
\nabla_3^{(1)} & \nabla_3^{(2)}
\end{pmatrix}, \hspace{3mm} 
S = \begin{pmatrix}
1 & 1  \\
1 & 1 \\
1 & 0
\end{pmatrix}, \hspace{3mm} \Pi = \begin{pmatrix}
\pi^{(1)} & \pi^{(2)} \\
\pi^{(1)} & \pi^{(2)} \\
\pi^{(1)} & \pi^{(2)}
\end{pmatrix}, \hspace{3mm} 
\Omega = \begin{pmatrix}
\pi^{(1)} + \pi^{(2)} \\
\pi^{(1)} + \pi^{(2)} \\
\pi^{(1)}
\end{pmatrix} \\
J_\beta &= \begin{pmatrix}
\big(\pi^{(1)} + \pi^{(2)}\big)^{-1} [\pi^{(1)}\nabla_1^{(1)} + \pi^{(2)}\nabla_1^{(2)}] \\
\big(\pi^{(1)} + \pi^{(2)}\big)^{-1}[\pi^{(1)}\nabla_2^{(1)} + \pi^{(2)}\nabla_2^{(2)}] \\
\big(\pi^{(1)}\big)^{-1}[\pi^{(1)}\nabla_3^{(1)}] 
\end{pmatrix} \\
P_\beta &= \begin{pmatrix}
(1-\alpha)\lambda \beta_1 + \alpha\lambda g_1 \\
(1-\alpha)\lambda \beta_2 + \alpha\lambda g_2 \\
(1-\alpha)\lambda \beta_3 + \alpha\lambda g_3
\end{pmatrix}
\end{align*}
$$

Putting it all together, we can write the (sub) gradient of our multitask loss function in vectorized form:

$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta} &= \frac{1}{N} J_\beta \iota_p + P_\beta \iota_p
\end{align*}
$$

### (2.A) Transfer learning approach #1

Now we'll define $T$ to refer dataset of interest (the target) and $S$ to refer to all other datasets. We can then apply a scaling factor $\tau$ to represent how close we want to be to the multitask situation where $\tau=0$ is pure multitask and $\tau=1$ is only estimating our target dataset: $\pi^{(S)}(1-\tau) + \pi_T = 1$. All that changes is the $\pi$ vector.

$$
\begin{align*}
\pi &= \begin{bmatrix} (1-\tau)\pi^{(1)} & \cdots  & (1-\tau)\pi^{(D_1)} & 1-(1-\tau)\pi_T \end{bmatrix}^T
\end{align*}
$$


Consider the general L2 penalized multitask learning function to be minimized, where there are $D$ different datasets, and $p$ total parameters $\beta \in \mathbb{R}^p$. Each dataset has a loss function $l^{(d)}(\cdot)$, and an associated parameter vector $\beta^{(d)}$, which may be a subset of the parameter space: $\beta_d \subseteq \beta$.

$$
\begin{align*}
l_M(\beta) &= \frac{1}{N} \sum_{d=1}^D \Bigg\{ \pi^{(d)} \cdot l^{(d)}(\beta^{(d)})  \Bigg\} + \frac{1}{2} \lambda \|\Gamma \beta \|_2^2 \\
\pi^{(d)} &= \frac{N^{(d)}}{N}, \hspace{3mm} N = \sum_{i=1}^D N^{(d)}
\end{align*}
$$

The Tikhonov matrix $\Gamma$ is needed in order to ensure that coefficients are not unduly regularized simply because they are only found in a subset of loss functions. Specifically define:

$$
\begin{align*}
\Gamma &= \text{diag}\Bigg(\frac{N_{\beta_1}}{N},\dots,\frac{N_{\beta_p}}{N} \Bigg)^{1/2} \\
\|\Gamma \beta \|_2^2 &= \sum_{j=1}^p \frac{N_{\beta_j}}{N} \beta_j^2 \\
N_{\beta_j} &= \sum_{d \in S_j} N^{(d)}, \hspace{2mm} S_j = \{i: \beta_j \in \beta^{(i)} \}
\end{align*}
$$

For example, suppose there were two datasets $\{1,2\}$ and three parameters $\beta=(\beta_1,\beta_2,\beta_3)'$ of the following form:

$$
\begin{align*}
l_M(\beta) &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot l(\beta^{(d)})  \Bigg\} +  \frac{\lambda}{2} \sum_{j=1}^3 \frac{N_{\beta_j}}{N} \beta_j^2 \\
\beta^{(1)} &= (\beta_1,\beta_2,\beta_3)^T \\
\beta^{(2)} &= (\beta_1,\beta_2)^T \\
N_{\beta_1} &= N, \hspace{2mm} N_{\beta_2} = N, \hspace{2mm} N_{\beta_3} = N^{(1)}
\end{align*}
$$

We can consider the KKT conditions to see why this form of Tikhonov regularization is useful:


$$
\begin{align*}
\frac{\partial l_M(\beta)}{\partial \beta_1} &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_1} l(\beta^{(d)})  \Bigg\} + \lambda \beta_1 \\
\frac{\partial l_M(\beta)}{\partial \beta_2} &= \frac{1}{N} \sum_{d=1}^2 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_2} l(\beta^{(d)})  \Bigg\} + \lambda \beta_2 \\
\frac{\partial l_M(\beta)}{\partial \beta_3} &= \frac{1}{N} \sum_{d=1}^1 \Bigg\{ \pi^{(d)} \cdot \nabla_{\beta_3} l(\beta^{(d)})  \Bigg\} + \big(\pi^{(1)}\big) \lambda \beta_3
\end{align*}
$$

### (1.A) Extension to elastic net type penalty

TL has become increasingly popular in machine learning (ML). Most of the cases involve re-using existing neural networks weights from a previous task as a starting point for fitting a new neural network to a smaller domain of data. This can be done by literally taking the entire network weights as initial values and then updating them with the new data, or more reasonably, randomizing the last several layers of nodes and then retraining those. For example, [Matlab](https://www.mathworks.com/help/nnet/examples/transfer-learning-using-alexnet.html) recently included functionality to "transfer" the trained AlexNet Deep Convolutional NN to new image classification problems. TL has also been endoresed by Andrew NG, who, according to those who were at the 2016 NIPS conference, said that TL will be next succeed supervised machine learning in commercial succes.

<br>
<h4> <p align="center"> Figure 1: With TL take over? </p> </h4>
<p align="center"><img src="figures/transfer_ng.png" width="60%"></p>
<br>

This is all very interesting and makes sense for image classification with DNNs since many of their hidden node layers have "learned" to recognize certain facial features. For example Figure 2 shows how a convolutional input is translated as into different features by the DNN. Some of these low/medium level features, for example, could be useful even for pictures of cats, since their faces have these low-levels properties too. 

<br>
<h4> <p align="center"> Figure 2: TL makes sense for images </p> </h4>
<p align="center"><img src="figures/face_patterns.png" width="80%"></p>
<p align="center"> Source: [Sebastian Ruder](http://ruder.io/transfer-learning/index.html)</p>
<br>

As a trained statistician, I'm more interested in thinking about TL for very basic regression problems to build some simple algebraic intuition. 



$$
\begin{align*}
\hat{\beta}_0 &= \log\Bigg(\frac{\phi_{n,m}}{1-\phi_{n,m}} \Bigg), \hspace{3mm} \phi_{n,m}=\frac{n}{n-m}\bar{y}_n - \frac{m}{n-m}\bar{y}_m \\
\hat{\beta_1} &= \log\Bigg(\frac{\bar{y}_m}{1-\bar{y}_m} \Bigg) - \hat{\beta}_0
\end{align*}
$$

```{r}
n=100;b0=-1;b1=2
x <- rbinom(n,1,0.5)
y <- rbinom(n,1,1/(1 + exp(-(b0+b1*x))))
m <- sum(x==1)
ym <- mean(y[x==1])
yn <- mean(y)
phi.nm <- (n/(n-m))*yn - (m/(n-m))*ym
b0.hat <- log(phi.nm/(1-phi.nm))
b1.hat <- log(ym/(1-ym)) - b0.hat
data.frame(glm=round(coef(glm(y~x,family=binomial)),3),hand=round(c(b0.hat,b1.hat),3))
```

---
title: 'Introduction to the EM Algorithm'
output: html_document
fontsize: 12pt
published: true
status: no_process
mathjax: true
---

$$
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bz}{\boldsymbol{z}}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\bSig}{\boldsymbol{\Sigma}}
$$

## Introduction

The Expectation-Maximization (EM) algorithm is designed to find the parameters that (locally) maximize a likelihood function in the presence of latent variables. At a high level, the algorithm alternates between updating the latent variables in the E step for a fixed set of parameters, and then updating the parameters in the M step by maximizing the log-likelihood while fixing the latent variables. For mixture models, where the unconditional joint distribution is a sum of distributions, the EM algorithm proves highly effective at solving some fairly complex likelihoods. In the rest of this post is as follows: (1) the K-Means algorithm is used to build some intuition, (2) a simple Gaussian Mixture model is used, and (3) a more complex micture cure model from survival analysis is estimated.

## (2) Gaussian Mixture model

Suppose that the unconditional distribution of $p$-dimensional vector $\bx$ was assumed to be composed to a linear combination of $K$ Gaussian distributions. Algebraically this has a simple form for the pdf of $\bx$:

$$
\begin{align*}
f(\bx) &= \pi_1 f_1(\bx) + \dots + f_K(\bx) \\
\sum_{k=1}^K \pi_k &= 1, \hspace{3mm} f_k \sim N(\bmu_k,\bSig_k), \hspace{3mm} \pi_k = p(z_k=1)
\end{align*}
$$

It's easy enough to see that a Gaussian mixture distribution has a valid CDF. Now consider a $K$ dimensional vector $\bz$, where only one element is 1, the rest are zero: $z_k \in \{0,1\}$ and $\sum_k z_k=1$. Because $\bz$ uses a 1-of-$K$ representation it has a pmf which follows the multinomial distribution with one trial:

$$
\begin{align*}
p(\bz) &= \prod_{k=1}^K \pi_k^{z_k}
\end{align*}
$$

With the conditional distribution of $\bx$ following a Gaussian for a given $z_k=1$:

$$
\begin{align*}
f(\bx|z_k=1) &= \N(\bx|\bmu_k,\bSig_k)^{z_k} \\
f(\bx|\bz) &= \prod_{k=1}^K \N(\bx|\bmu_k,\bSig_k)^{z_k}
\end{align*}
$$


Using the law of total probability we see that:

$$
\begin{align}
f(\bx) &= \sum_{z\in \mathcal{Z}} f(\bx|\bz) p(\bz) \nonumber \\
&= \sum_{z\in \mathcal{Z}} \Bigg\{  \prod_{k=1}^K \N(\bx|\bmu_k,\bSig_k)^{z_k}  \pi_k^{z_k} \Bigg\} \nonumber \\
&= \sum_{k=1}^K \pi_k \N(\bx|\bmu_k,\bSig_k) \label{eq:gauss_mix}
\end{align}
$$

Where $\N$ is the Gaussian density function. Equation \eqref{eq:gauss_mix} has an intuitive form: the density of the random vector $\bx$ is the probability-weighted sum of Gaussian densities. For the case where there are $N$ observations: $\{\bx_1,\dots,\bx_N\}$ there will be $N$ latent variables $\{\bz_1,\dots,\bz_N\}$. Thus the Gaussian mixture model is now redolent of the K-Means problem where we had to assign each observation to a different cluster, although now each observation has an explicit probability associated for each Gaussian cluster $\pi_{ik}$. However the same principal will apply when applying the EM algorithm: conditional on a set of mean and covariance matrices, update the probability of each observation to each Gaussian distribution (E-step), and then conditional on these probability weights, update the Gaussian mean and covariances using the assigned observations.

In the K-means case, the E-step cluster assignment was easy: we assigned $\bx$ to whichever cluster was the closest via Euclidian distance. If $\pi_k$ represents the prior probability that $\bx$ is drawn from the $k^{th}$ Gaussian distribution, then we can think of its posterior as the probability of that $z_k=1$ given the observed $\bx$, which will be defined as:

$$
\begin{align*}
\gamma(z_k) &= p(z_k=1|\bx) \\
&= \frac{p(\bx|z_k=1)p(z_k=1)}{p(\bx)} \\
&= \frac{p(\bx|z_k=1)p(z_k=1)}{\sum_{z \in \mathcal{Z}} p(\bx|z_k=1)p(z_k=1) } \\
&= \frac{\pi_k\N(\bx|\bmu_k,\bSig_k)}{\sum_{j=1}^K \pi_j\N(\bx|\bmu_j,\bSig_k)}
\end{align*}
$$

Where the above derivations rely on Bayes rule: $P(B|A)=P(A|B)P(B)/P(A)$. In Bishop's terminology we say that $\gamma(\pi_k)$ is the "responsibility that component $k$ takes for 'explaining' the observation $\bx$". The log-likelihood of a density of $N$ observations is fairly simple:

$$
\begin{align*}
\log f(\bX |\bpi,\bmu,\bSig) &= \sum_{i=1}^N \log \Bigg\{ \sum_{k=1}^K \pi_k\N(\bx|\bmu_k,\bSig_k)  \Bigg\}
\end{align*}
$$


### Example: Old Faithful dataset




## (3) Mixture Cure Model









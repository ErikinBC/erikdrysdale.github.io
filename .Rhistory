data <- rnorm(10)
mean(data)
median(data)
library(genefilter)
matrix(rnorm(1000),ncol=100)
genefilter::colttests(matrix(rnorm(1000),ncol=100),fac=rep(c('a','b'),5))
genefilter::colttests(matrix(rnorm(1000),ncol=100) %>% data.frame,fac=rep(c('a','b'),5))
library(magrittr)
genefilter::colttests(matrix(rnorm(1000),ncol=100) %>% data.frame,fac=rep(c('a','b'),5))
rep(c('a','b'),5)
genefilter::colttests(matrix(rnorm(1000),ncol=100) %>% data.frame,fac=rep(c('a','b'),5) %>% factor)
rep(c('a','b'),5) %>% factor
:colttests(matrix(rnorm(1000),ncol=100) %>% data.frame
genefilter::colttests(matrix(rnorm(1000),ncol=100),fac=rep(c('a','b'),5) %>% factor)
tests <- genefilter::colttests(matrix(rnorm(1000),ncol=100),fac=rep(c('a','b'),5) %>% factor)
hist(tests$statistic)
tests$statistic
?cut
cut(tests$statistic,breaks=c(-4,-2,0,2,4))
cut(tests$statistic,breaks=c(-4,-2,0,2,4)) %>% table
fisher.table <- data.frame('Twins'=c('Identical Twins','Fraternal Twins'),
'Like Habits'=c(44,9),
'Unlike habits'=c(9,9),
'Total'=c(53,18))
# Calculate the two probabilities
fisher.prop <- fisher.table[,2:3] %>% divide_by(rowSums(.))
library(magrittr)
fisher.table <- data.frame('Twins'=c('Identical Twins','Fraternal Twins'),
'Like Habits'=c(44,9),
'Unlike habits'=c(9,9),
'Total'=c(53,18))
# Calculate the two probabilities
fisher.prop <- fisher.table[,2:3] %>% divide_by(rowSums(.))
pi1 <- fisher.prop[1,1]
pi2 <- fisher.prop[2,1]
# Get ni/yi
y1 <- fisher.table[1,2]
y2 <- fisher.table[2,2]
n1 <- fisher.table[1,4]
n2 <- fisher.table[2,4]
fisher.mat <- fisher.table[1:2,2:3]
y11 <- fisher.mat[1,1]
y12 <- fisher.mat[1,2]
y21 <- fisher.mat[2,1]
y22 <- fisher.mat[2,2]
n1 <- y11 + y12
n2 <- y21 + y22
m1 <- y11 + y21
# Define the pdf
fe.pdf <- function(y11,n1,n2,m1) {
choose(n1,y11)*choose(n2,m1-y11)/choose(n1+n2,m1)
}
fisher.test(matrix(c(a,c,b,d),ncol=2),alternative='two.sided')$p.value
fisher.test(matrix(c(y11,y21,y12,y22),ncol=2),alternative='two.sided')$p.value
fisher.test(matrix(c(y11,y21,y12,y22),ncol=2),alternative='two.sided')$p.value
fisher.test(matrix(c(y11,y21,y12,y22),ncol=2),alternative='two.sided')$p.value
fisher.test(matrix(c(y11,y21,y12,y22),ncol=2),alternative='two.sided')$p.value
fisher.test(matrix(c(y11,y21,y12,y22),ncol=2),alternative='two.sided')$p.value
y11 <- fisher.mat[1,1]
y12 <- fisher.mat[1,2]
y21 <- fisher.mat[2,1]
y22 <- fisher.mat[2,2]
n1 <- y11 + y12
n2 <- y21 + y22
m1 <- y11 + y21
# Define the pdf
fe.pdf <- function(y11,n1,n2,m1) {
choose(n1,y11)*choose(n2,m1-y11)/choose(n1+n2,m1)
}
fe.pdf(y11,n1,n2,m1)
bpv <- fe.pdf(y11,n1,n2,m1)
n1
fe.pdf(1:2,n1,n2,m1)
rpv <- fe.pdf(seq(1,n1),n1,n2,m1)
which(rpv <= bpv)
rpv[which(rpv <= bpv)]
sum(rpv[which(rpv <= bpv)])
fisher.exact <- sum(rpv[which(rpv <= bpv)])
gamma(5)
prod(1:5)
prod(1:4)
library(balanceHD); library(dplyr); library(magrittr)
# MC parameters (from Github)
n = 400
p = 1000
tau = 7
nclust = 10
beta = 2 / (1:p) / sqrt(sum(1/(1:p)^2))
clust.ptreat = rep(c(0.1, 0.9), nclust/2)
# Test that X is not correlated with D
XD.test <- replicate(500,{
cluster.center = 0.5 * matrix(rnorm(nclust * p), nclust, p)
cluster = sample.int(nclust, n, replace = TRUE)
X = cluster.center[cluster,] + matrix(rnorm(n * p), n, p)
W = rbinom(n, 1, clust.ptreat[cluster])
apply(X,2,function(col) cor(W,col))
})
XD.test %>% apply(1,mean)
# Modify how W is created and test
wager.sim <- function(ss) {
set.seed(ss)
n <- 100
p <- 250
tau <- 7
nclust <- 10
beta <- 2 / (1:p) / sqrt(sum(1/(1:p)^2))
clust.ptreat <- rep(c(0.1, 0.9), nclust/2)
cluster.center <- 0.5 * matrix(rnorm(nclust * p), nclust, p)
cluster <- sample.int(nclust, n, replace = TRUE)
X <- cluster.center[cluster,] + matrix(rnorm(n * p), n, p)
# Make W correlated with X
Xbeta <- X %*% beta
p.W <- (Xbeta-min(Xbeta))/(max(Xbeta)-min(Xbeta))
W <- rbinom(n, 1, p.W)
Y <- X %*% beta + rnorm(n, 0, 1) + tau * W
# ARB model
tau.hat <- residualBalance.ate(X, Y, W, estimate.se = T)
# OLS
ols <- coef(lm(Y~-1+W+.,data=data.frame(Y,W,X[,1:25])))[1]
return(c(tau.hat,ols))
}
# Simulate
n.sim <- 500
wager.store <- tibble(theta=rep(NA,n.sim),se=rep(NA,n.sim),ols=rep(NA,n.sim))
for (k in 1:n.sim) {
if(mod(k,100)==0) { print(k) }
wager.store[k,] <- wager.sim(k)
}
# OLS not biased by RBA is
apply(wager.store,2,mean)
# Modify how W is created and test
arb.sim <- function(ss) {
set.seed(ss)
n <- 100
p <- 250
tau <- 7
nclust <- 10
beta <- 2 / (1:p) / sqrt(sum(1/(1:p)^2))
clust.ptreat <- rep(c(0.1, 0.9), nclust/2)
cluster.center <- 0.5 * matrix(rnorm(nclust * p), nclust, p)
cluster <- sample.int(nclust, n, replace = TRUE)
X <- cluster.center[cluster,] + matrix(rnorm(n * p), n, p)
# Make W correlated with X
Xbeta <- X %*% beta
p.W <- (Xbeta-min(Xbeta))/(max(Xbeta)-min(Xbeta))
W <- rbinom(n, 1, p.W)
Y <- X %*% beta + rnorm(n, 0, 1) + tau * W
# ARB model
tau.hat <- residualBalance.ate(X, Y, W, estimate.se = T)
# OLS
ols <- coef(lm(Y~-1+W+.,data=data.frame(Y,W,X[,1:25])))[1]
return(c(tau.hat,ols))
}
# Simulate
n.sim <- 500
arb.store <- tibble(theta=rep(NA,n.sim),se=rep(NA,n.sim),ols=rep(NA,n.sim))
for (k in 1:n.sim) {
if(mod(k,100)==0) { print(k) }
arb.store[k,] <- arb.sim(k)
}
apply(wager.store,2,na.mean)
apply(wager.store,2,mean)
tail(wager.store)
arb.store
arb.store %>% apply(1,mean)
arb.store %>% apply(2,mean)
arb.store %>% apply(2,mean,na.rm=T)
load('C:/Users/erikinwest/Documents/Courses/Project/rmd_data.RData')
gg.arb2
rmd.list$gg.arb2
rmd.list$gg.arb1
plot(iris$Sepal.Length)
dev.off()
gg.dml
rmd.list$gg.arb2 + labs(x='test')
ll <- c('tidyverse','magrittr','cowplot','scales')
sapply(ll,function(l) require(l,character.only = T))
rmd.list$gg.arb2 + labs(x='test')
gg.arb2 + labs(x=expression(hat(tau)))
dev.off()
options(max.print = 100)
ll <- c('tidyverse','magrittr','forcats','stringr','cowplot','broom','scales','reshape2','ggrepel',
'MASS','glmnet')
sapply(ll,library,character.only=T)
dir.base <- 'C:/Users/erikinwest/Documents/Courses/Project/'
rm(list=ls())
setwd(dir.base)
rm(list=ls())
dir.base <- 'C:/Users/erikinwest/Documents/Courses/Project/'
setwd(dir.base)
# Volume o fa p-ball with raidues r propto r^p, for some radiues r, hence the probability than a given point x_1
# is less than a distance r is r^p.
# Radiues length of the hyperball to capture a fraction r of the volume of the data
# Seet it
set.seed(1)
# The probability than N points will have a distance less than r (radius) or a hypercube
pcube.cdf <- function(r,p,n) { 1-(1-r^p)^n }
# Different dimensions
p <- c(1,5,10,250)
# Create some data
r.seq <- seq(0,1,0.001)
r.dat <- data.frame(matrix(rep(r.seq,4),ncol=4)) %>% set_colnames(p) %>%
tbl_df %>% gather %>% group_by(key) %>% mutate(cdf=pcube.cdf(r=value,p=as.numeric(key),n=100))
# Get the approximate median vertical points
r.vlines <- r.dat %>% group_by(key) %>% filter(cdf>=0.5) %>%
summarise(mv=min(value)) %>% mutate(key=fct_reorder(key,as.numeric(key)))
# Theoretical distribution
gg.theoretical <-
ggplot(r.dat,aes(x=value,y=cdf,color=fct_reorder(key,as.numeric(key)))) +
geom_line(size=1.5) + scale_color_discrete(name='Dimension') +
theme(legend.position = 'bottom') +
labs(x='Radius from origin',y='Prob. of one obs.',
subtitle='In p-ball of radius 1\nVertical lines shows median estimate') +
geom_vline(xintercept=r.vlines$mv,color='black',linetype=2)
# Steal the legend
horiz.leg <- get_legend(gg.theoretical)
gg.theoretical <- gg.theoretical + theme(legend.position = 'none')
# Simulate some data for a given n and p
X.min <- function(n,p) {
# P(D<r) = (r/R_s)^p, so D=R_s* U^(1/p), U~Unif(0,1)
D <- runif(n=100)**(1/p)
# Generate some random normals from our dimension
X <- matrix(rnorm(n*p),ncol=p)
# Get norm of each row and the distance to get the normalize value
X.norm <- apply(X,1,function(rr) sqrt(sum(rr^2)))
# Get the final points
X.final <- X * matrix(rep(D/X.norm,p),ncol=p)
# Get euclidian distance from original and return the minimum
X.min <- apply(X.final,1,function(rr) sqrt(sum(rr^2))) %>% min
return(X.min)
}
# Simulate some data
dist.sim <- do.call('cbind',lapply(p,function(p1) replicate(1000,{X.min(n=100,p=p1)}) %>%
data.frame %>% set_colnames(paste('p',p1,sep='='))) ) %>% tbl_df
# Gather
dist.gather <- dist.sim %>% gather %>% mutate(key=gsub('p=','',key),key=fct_reorder(key,as.numeric(key)))
# gg it
gg.sim <- ggplot(dist.gather,aes(x=value)) +
geom_density(aes(fill=key),color='black',alpha=0.75) +
labs(x='Minimum distance',y='Density',subtitle='Monte carlo simunlations\nVertical lines shows median estimate') +
theme(legend.position = 'none') +
facet_wrap(~key,scales='free',ncol=2) +
geom_vline(data=r.vlines,aes(xintercept=mv),color='black',linetype=2)
# Dist example
gg.ptheory <- plot_grid(ggplot() + draw_label('Closest point in P-ball (N=100)'),
plot_grid(gg.theoretical,gg.sim,nrow=1,labels=c('A','B')),
horiz.leg,ncol=1,rel_heights = c(2,10,1))
# --- RIDGE VERSUS LASSO --- #
# Function to generate our standard data set
dgp <- function(ss,r) {
set.seed(ss)
n.train <- 100
n.test <- 1000
n <- n.train + n.test
p.max <- 250
p.seq <- 1:p.max
# Generate all data
X.all <- rnorm(n*p.max) %>% matrix(ncol=p.max)
# Generate p betas with information approaching 0 at 1/sqrt(n)
beta <- rep(NA,p.max)
for (k in p.seq) {  beta[k] <- rnorm(1,mean=0,sd=(1/k)^r) }
# and some white noise
eps <- rnorm(n)
# Generate the y's
y.all <- (X.all %*% beta) + eps
# Use the first 100 observations as training dat
rid <- 1:100
Xy.train <- data.frame(y=y.all[rid,],X.all[rid,])
Xy.test <- data.frame(y=y.all[-rid,],X.all[-rid,])
# Return
return(list(Xy.train=Xy.train,Xy.test=Xy.test,tbeta=beta))
}
# Generate some data
some.dat <- dgp(ss=1,r=1)
# Find the best fitting Ridge
l.seq <- seq(50,1000,1)
some.ridge <- lm.ridge(y~.-1,data=some.dat$Xy.train,lambda = l.seq)
# Get the minimum one...
lam.ridge <- l.seq[which(some.ridge$GCV==min(some.ridge$GCV))]
coef.ridge <- coef(lm.ridge(y~.-1,data=some.dat$Xy.train,lambda = lam.ridge))
# use CV for lasso
cv.lasso <- cv.glmnet(x=some.dat$Xy.train[,-1] %>% as.matrix,
y=some.dat$Xy.train[,1],alpha=1,intercept=F)
# Fit with the lambda-minimizin
lam.lasso <- cv.lasso$lambda.min
some.lasso <- glmnet(x=some.dat$Xy.train[,-1] %>% as.matrix,y=some.dat$Xy.train[,1],
alpha=1,lambda=lam.lasso,intercept=F)
# Get the coefficients
coef.lasso <- as.matrix(some.lasso$beta) %>% as.vector
# Long form it
coef.compare <- data.frame(true=some.dat$tbeta,Ridge=coef.ridge,LASSO=coef.lasso) %>%
tbl_df %>% gather(model,val,-true)
# Plot it
gg.coef.compare <- ggplot(coef.compare,aes(x=val,y=true)) +
geom_point(size=1.5,aes(color=model),show.legend = F) +
labs(x='Penalized coefficient',y='True coefficient') +
scale_color_discrete(name='Model: ') +
facet_wrap(~model,ncol=1) +
background_grid(major='xy',minor='none') +
theme(legend.position = 'bottom') +
geom_abline(intercept = 0,slope = 1,color='black')
# Show the solution path for the LASSO
lam.path <- seq(0.001,lam.lasso,length.out = 50)
lasso.path <- glmnet(x=some.dat$Xy.train[,-1] %>% as.matrix,y=some.dat$Xy.train[,1],
alpha=1,lambda=lam.path,intercept=F)
# Long form
long.path <- lasso.path$beta %>% as.matrix %>% t %>% cbind(lam=lasso.path$lambda,.) %>% data.frame %>% gather(var,val,-lam)
# First plot the solution path
gg.path1 <- ggplot(long.path %>% filter(val>0),aes(x=lam,y=val,color=var)) +
geom_line(show.legend = F) +
labs(x=expression(lambda),y=expression(beta[k]),subtitle='LASSO solution path')
# Then count the number of non-zero coefficients...
nbeta <- long.path %>% group_by(lam) %>% summarise(nbeta=sum(val>0))
# %>% mutate(nd=nbeta-lag(nbeta)) %>% filter(nd!=0 | is.na(nd))
gg.path2 <- ggplot(nbeta,aes(x=lam,y=nbeta)) +
geom_line() + geom_point() +
labs(subtitle='# of non-zero coef.',y='') +
theme(axis.title.x=element_blank(),axis.text.x = element_blank(),
axis.ticks.x = element_blank(),axis.title.y = element_blank())
# Combine
gg.lasso.path <- plot_grid(gg.path1,gg.path2,ncol=1,rel_heights = c(2,1),align = 'v')
lam.path
glmnet
test <- glmnet(x=some.dat$Xy.train[,-1] %>% as.matrix,y=some.dat$Xy.train[,1],
alpha=1,lambda=lam.path,intercept=F)
test$beta
as.matrix(test$beta) %>% head
as.matrix(test$beta) %>% dim
test <- glmnet(x=some.dat$Xy.train[,-1] %>% as.matrix,y=some.dat$Xy.train[,1],
alpha=1,lambda=c(0.001,lam.lasso),intercept=F)
as.matrix(test$beta) %>% dim
as.matrix(test$beta)
nbeta
nbeta %>% tail
rm(list=ls())
arb.sim <- function(ss) {
set.seed(ss)
n <- 400
p <- 1000
tau <- 7
nclust <- 10
beta <- 2 / (1:p) / sqrt(sum(1/(1:p)^2))
clust.ptreat <- rep(c(0.1, 0.9), nclust/2)
cluster.center <- 0.5 * matrix(rnorm(nclust * p), nclust, p)
cluster <- sample.int(nclust, n, replace = TRUE)
X <- cluster.center[cluster,] + matrix(rnorm(n * p), n, p)
# Make W correlated with X
Xbeta <- X %*% beta
p.W <- (Xbeta-min(Xbeta))/(max(Xbeta)-min(Xbeta))
W <- rbinom(n, 1, p.W)
Y <- X %*% beta + rnorm(n, 0, 1) + tau * W
# ARB model
tau.hat <- residualBalance.ate(X, Y, W, estimate.se = T)
# OLS
ols <- coef(lm(Y~-1+W+.,data=data.frame(Y,W,X[,1:25])))[1]
return(c(tau.hat,ols))
}
n.sim <- 500
arb.store <- tibble(theta=rep(NA,n.sim),se=rep(NA,n.sim),ols=rep(NA,n.sim))
for (k in 1:n.sim) {
if(mod(k,100)==0) { print(k) }
arb.store[k,] <- arb.sim(k)
}
apply(arb.store,2,mean)
log(400/1000
log(400)/1000
log(1000)/4000
log(1000)/400
log(250)/100
ggplot + draw_label('Figure 5: LASSO model',fontface = 'bold',size=14)
ll <- c('tidyverse','magrittr','cowplot','scales')
sapply(ll,function(l) require(l,character.only = T))
#' This R script will process all R mardown files (those with in_ext file extention,
#' .rmd by default) in the current working directory. Files with a status of
#' 'processed' will be converted to markdown (with out_ext file extention, '.markdown'
#' by default). It will change the published parameter to 'true' and change the
#' status parameter to 'publish'.
#'
#' @param path_site path to the local root storing the site files
#' @param dir_rmd directory containing R Markdown files (inputs)
#' @param dir_md directory containing markdown files (outputs)
#' @param url_images where to store/get images created from plots directory +"/" (relative to path_site)
#' @param out_ext the file extention to use for processed files.
#' @param in_ext the file extention of input files to process.
#' @param recursive should rmd files in subdirectories be processed.
#' @return nothing.
#' @author Jason Bryer <jason@bryer.org> edited by Andy South
# setwd("C:/Users/erikinwest/Documents/bioeconometrician/github/erikdrysdale.github.io/")
# path_site = getwd();dir_rmd = "_rmd";dir_md = "_posts"
# url_images = "figures/";out_ext='.md';in_ext='.rmd';recursive=FALSE
rm(list=ls())
rmd2md <- function( path_site = getwd(),
dir_rmd = "_rmd",
dir_md = "_posts",
#dir_images = "figures",
url_images = "figures/",
out_ext='.md',
in_ext='.rmd',
recursive=FALSE) {
require(knitr, quietly=TRUE, warn.conflicts=FALSE)
#andy change to avoid path problems when running without sh on windows
files <- list.files(path=file.path(path_site,dir_rmd), pattern=in_ext, ignore.case=TRUE, recursive=recursive)
for(f in files) {
message(paste("Processing ", f, sep=''),encoding = "UTF-8")
content <- readLines(file.path(path_site,dir_rmd,f))
# If any html calls, replace the src=figures/... with src=/figures/...
src.idx <- grep('src=',content,value=F)
if (length(src.idx)>0) {
content[src.idx] <- gsub('src=\"figures','src=\"/figures',content[src.idx])
} else {}
frontMatter <- which(substr(content, 1, 3) == '---')
if(length(frontMatter) >= 2 & 1 %in% frontMatter) {
statusLine <- which(substr(content, 1, 7) == 'status:')
publishedLine <- which(substr(content, 1, 10) == 'published:')
if(statusLine > frontMatter[1] & statusLine < frontMatter[2]) {
status <- unlist(strsplit(content[statusLine], ':'))[2]
status <- sub('[[:space:]]+$', '', status)
status <- sub('^[[:space:]]+', '', status)
if(tolower(status) == 'process') {
#This is a bit of a hack but if a line has zero length (i.e. a
#black line), it will be removed in the resulting markdown file.
#This will ensure that all line returns are retained.
content[nchar(content) == 0] <- ' '
message(paste('Processing ', f, sep=''))
content[statusLine] <- 'status: publish'
content[publishedLine] <- 'published: true'
#andy change to path
outFile <- file.path(path_site, dir_md, paste0(substr(f, 1, (nchar(f)-(nchar(in_ext)))), out_ext))
#render_markdown(strict=TRUE)
#render_markdown(strict=FALSE) #code didn't render properly on blog
#andy change to render for jekyll
render_jekyll(highlight = "pygments")
#render_jekyll(highlight = "prettify") #for javascript
opts_knit$set(out.format='markdown')
# andy BEWARE don't set base.dir!! it caused me problems
# "base.dir is never used when composing the URL of the figures; it is
# only used to save the figures to a different directory.
# The URL of an image is always base.url + fig.path"
# https://groups.google.com/forum/#!topic/knitr/18aXpOmsumQ
# Get data directory
opts_knit$set(root.dir = dir_rmd)
opts_knit$set(base.url = "/")
# opts_knit$set(fig.width = 10)
opts_chunk$set(fig.path = url_images)
# opts_chunk$set(fig.width = 10)
#andy I could try to make figures bigger
#but that might make not work so well on mobile
opts_chunk$set(fig.width  = 8.5,
fig.height = 7.5,
dpi=300)
try(knit(text=content, output=outFile,encoding = "UTF-8"), silent=FALSE)
} else {
warning(paste("Not processing ", f, ", status is '", status,
"'. Set status to 'process' to convert.", sep=''))
}
} else {
warning("Status not found in front matter.")
}
} else {
warning("No front matter found. Will not process this file.")
}
}
invisible()
}
setwd("C:/Users/erikinwest/Documents/bioeconometrician/github/erikdrysdale.github.io/")
rmd2md()
# c1 <- 'black'
# g1 <- ggplot(gather(iris,var,val,-Species) %>% tbl_df,aes(x=val,y=..density..)) +
#   geom_density(aes(fill=Species),color=c1) + facet_wrap(~var)
#
# # Save data in a list
# rmd.list <- list(g1=g1,c1=c1)
# save(rmd.list,file='C:/Users/erikinwest/Documents/bioeconometrician/github/erikdrysdale.github.io/_rmd/rmd_data_test.RData')
# t1 <- readLines( "C:/Users/erikinwest/Documents/bioeconometrician/github/erikdrysdale.github.io/_posts/2016-12-28-batch_effects.md")
# t2 <- readLines( "C:/Users/erikinwest/Documents/bioeconometrician/github/erikdrysdale.github.io/_posts/2016-12-28-old_batch_effects.md")
#
# t1[which(!t1==t2)]
# t2[which(!t1==t2)]
rmd2md()
plot_grid(ggplot() + draw_label('Figure 6: Double machine learning '),
gg.dml + theme_cowplot(font_size = 12),ncol=1,rel_heights = c(1,7))
plot_grid(ggplot() + draw_label('Figure 6: Double machine learning '),
gg.dml + theme_cowplot(font_size = 12) + theme(legend.position = 'none'),
ncol=1,rel_heights = c(1,7))
plot_grid(ggplot() + draw_label('Figure 6: Double machine learning '),
gg.dml + theme_cowplot(font_size = 10) + theme(legend.position = 'none'),
ncol=1,rel_heights = c(1,7))
rmd2
rmd2md()
dev.off()
rmd2md()
gg.arb2 + theme_cowplot(font_size=10) + labs(x=expression(hat(tau)),subtitle='Figure 7: ARB estimator')
gg.cv <- plot_grid(gg.cv.oos2 + theme_cowplot(font_size = 10),gg.cv.oos1+ theme_cowplot(font_size = 10),rel_widths = c(2,3),nrow=1,labels=c('A','B'))
plot_grid(ggplot() + draw_label('Figure 3: Cross-validation',fontface = 'bold',size = 14),
gg.cv, ncol = 1,rel_heights = c(1,7))
gg.cv <- plot_grid(gg.cv.oos2 + theme_cowplot(font_size = 10) + theme(legend.position = c(0.3,0.6),legend.direction = 'horizontal'),
gg.cv.oos1+ theme_cowplot(font_size = 10),rel_widths = c(2,3),nrow=1,labels=c('A','B'))
plot_grid(ggplot() + draw_label('Figure 3: Cross-validation',fontface = 'bold',size = 14),
gg.cv, ncol = 1,rel_heights = c(1,7))
gg.cv <- plot_grid(gg.cv.oos2 + theme_cowplot(font_size = 10) + theme(legend.position = c(0.35,0.8),legend.direction = 'horizontal'),
gg.cv.oos1+ theme_cowplot(font_size = 10),rel_widths = c(2,3),nrow=1,labels=c('A','B'))
plot_grid(ggplot() + draw_label('Figure 3: Cross-validation',fontface = 'bold',size = 14),
gg.cv, ncol = 1,rel_heights = c(1,7))
rmd2md()
dev.off()
rmd2md()
library(knitr)
?knit
?opts_knit
rmd2md()
rmd2md()
rmd2md()
